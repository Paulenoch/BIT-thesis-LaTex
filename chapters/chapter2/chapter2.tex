\chapter{预备知识与相关工作}

本章综述支撑本文三项创新点的核心背景知识，并固定全篇统一的评测协议与指标定义。本章的目标是"收敛"：只讲支撑后续创新章节所必须的背景，并把统一的评测口径固定下来，后面每章实验直接引用，不再重复。

\section{四旋翼控制接口与任务抽象}

\subsection{坐标系与控制量定义}

本文采用东北天（ENU）右手坐标系作为世界坐标系。如图~\ref{fig:coord_frame}所示，无人机的位置与速度定义在世界坐标系下，姿态以四元数$q = [w, x, y, z]$表示机体坐标系相对于世界坐标系的旋转。

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
  >=Stealth, scale=0.9,
  axis/.style={->, thick},
]
% 世界坐标系
\node[font=\small\bfseries, color=blue!70, anchor=east] at (-0.8, 3.5) {世界坐标系 (World)};
\draw[axis, blue!70] (0,0) -- (3.0,0) node[right, font=\small] {$X$ (前进方向)};
\draw[axis, blue!70] (0,0) -- (0,3.0) node[left, font=\small] {$Z$ (竖直向上)};
\draw[axis, blue!70] (0,0) -- (-1.2,-1.2) node[below left, font=\small] {$Y$ (侧向)};

% 无人机简化图
\node[draw, fill=gray!20, rounded corners=2pt, minimum width=1.2cm, minimum height=0.4cm] (drone) at (6.0, 1.5) {};
\node[font=\scriptsize] at (6.0, 1.0) {四旋翼};

% 机体坐标系
\node[font=\small\bfseries, color=red!70] at (6.0, 3.8) {机体坐标系 (Body)};
\draw[axis, red!70] (6.0,1.5) -- (7.5,1.5) node[right, font=\small] {$x_b$};
\draw[axis, red!70] (6.0,1.5) -- (6.0,3.0) node[left, font=\small] {$z_b$};
\draw[axis, red!70] (6.0,1.5) -- (5.2,0.7) node[below left, font=\small] {$y_b$};

% 速度指令
\draw[->, very thick, green!60!black, dashed] (6.0,1.5) -- (8.0,2.8) node[right, font=\small, color=green!60!black] {$\mathbf{v}_{\text{cmd}} = [v^x, v^y, v^z]$};

% 姿态四元数标注
\node[draw, rounded corners=2pt, fill=yellow!10, font=\scriptsize, inner sep=3pt] at (3.2, -0.5) {姿态: $q_t = [w, x, y, z]$};
\end{tikzpicture}
\caption{世界坐标系与机体坐标系定义，以及速度指令接口}
\label{fig:coord_frame}
\end{figure}

策略网络在每个控制周期输出世界坐标系下的三维线速度指令$\mathbf{v}_t = [v^x_t, v^y_t, v^z_t] \in \mathbb{R}^3$，该指令由低层控制器（姿态环+电机混控）转化为电机转速执行。控制频率由策略推理速度决定，在本文硬件配置下可达毫秒级。经典四旋翼建模与控制理论可参见Mahony等\cite{Mahony2012QuadrotorSurvey}的综述。

\subsection{任务形式化}

本文研究的高速视觉避障任务形式化为序列决策问题。在每个控制周期$t$，策略$\pi_\theta$根据观测$o_t$输出控制动作$a_t$，形成闭环：
\begin{equation}
  \mathcal{M} = \langle \mathcal{O}, \mathcal{A}, \mathcal{T}, \mathcal{G}, \tau_{\max} \rangle
  \label{eq:task_tuple}
\end{equation}
其中$\mathcal{O}$为观测空间（深度图像$D_t \in \mathbb{R}^{60 \times 90}$与轻量状态$s_t = [q_t, \tilde{v}^{\text{target}}]$），$\mathcal{A}$为动作空间（世界坐标系下的速度指令$\mathbf{v}_t \in \mathbb{R}^3$），$\mathcal{T}$为由仿真器物理引擎决定的状态转移函数，$\mathcal{G}$为回合终止条件集合，$\tau_{\max} = \SI{40}{s}$为最大回合时长。

策略以序列历史为条件输出当前动作：
\begin{equation}
  a_t = \pi_\theta(o_{\le t}, s_{\le t}) = \pi_\theta(D_{\le t}, q_{\le t}, \tilde{v}^{\text{target}})
  \label{eq:policy}
\end{equation}

\subsection{控制回路与低层控制器假设}

本文的端到端策略工作在速度指令层级，将低层控制器视为黑盒。具体地，我们对低层控制器做以下假设：

\begin{enumerate}
  \item 一阶响应近似：低层控制器对速度指令的跟踪可近似为带延迟的一阶系统，即$\dot{\mathbf{v}}_{\text{actual}} = \frac{1}{\tau_c}(\mathbf{v}_{\text{cmd}} - \mathbf{v}_{\text{actual}})$，其中$\tau_c$为控制器时间常数（$\tau_c \approx \SI{50}{ms}$--$\SI{100}{ms}$）；
  \item 速度饱和：实际速度受物理限制不超过最大可达速度$v_{\max}$（在本文仿真环境中$v_{\max} \approx \SI{15}{m/s}$）；
  \item 姿态稳定性：低层控制器能够在策略输出的速度指令范围内保持姿态稳定，不发生失稳翻转。
\end{enumerate}

上述假设确定了策略网络的"控制权限边界"：策略不需要关心电机级细节，只需输出合理范围内的速度指令。这一假设在Flightmare仿真平台\cite{Song2021Flightmare}中由内置的PID/几何控制器\cite{Lee2010GeometricControl}保证。

\subsection{安全指标与任务完成条件}

本文采用"碰撞不终止回合"的评测设定，即无人机在碰撞后继续飞行。这一设定的统计学优势在于：（1）避免了碰撞终止导致的幸存者偏差（survivor bias）——若碰撞后立即终止，则高碰撞率策略的后续轨迹被截断，无法公平比较完整回合的统计特性；（2）能够同时统计碰撞率与成功率两个互补指标；（3）保留了碰撞事件的完整时间序列，支持更细粒度的碰撞事件分析（如碰撞持续时间、间隔分布等）。

回合终止条件包括：（1）无人机沿$X$轴飞行距离达到$\SI{58}{m}$--$\SI{60}{m}$（成功）；（2）飞行时长超过$\tau_{\max} = \SI{40}{s}$（超时，通常意味着策略因频繁碰撞而无法正常前进）。


\section{模仿学习与分布偏移：BC与DAgger}

\subsection{行为克隆（BC）}

行为克隆（Behavioral Cloning, BC）是端到端控制中最常用的训练范式\cite{Pomerleau1989ALVINN}：以专家策略$\pi^*$生成的状态--动作对$\{(o_t, a_t^*)\}$为监督信号，通过最小化策略输出与专家动作之间的损失进行离线学习：
\begin{equation}
  \mathcal{L}_{\text{BC}} = \mathbb{E}_{(o,a^*) \sim d_{\pi^*}} \left[ \ell(\pi_\theta(o), a^*) \right]
  \label{eq:bc_general}
\end{equation}
其中$d_{\pi^*}$为专家策略诱导的状态分布，$\ell(\cdot, \cdot)$为损失函数（本文采用均方误差MSE）。

BC的优势在于训练稳定、样本效率高、实现简单。在端到端控制文献中，从Pomerleau的ALVINN\cite{Pomerleau1989ALVINN}到NVIDIA自动驾驶\cite{Bojarski2016EndToEndNVIDIA}再到Codevilla等的条件模仿学习\cite{Codevilla2018EndToEndDriving}，BC一直是基础训练方法。Osa等\cite{Osa2018ImitationSurvey}对模仿学习的算法视角进行了全面综述。

\subsection{分布偏移与误差累积}

BC的核心问题在于闭环分布偏移（covariate shift）\cite{Ross2011DAgger}：训练数据由专家策略诱导的状态分布$d_{\pi^*}$生成，而部署时策略访问的状态分布$d_{\pi_\theta}$由学生策略自身诱导。当学生策略在某些状态下产生微小偏差$\epsilon$时，后续状态会偏离专家数据的覆盖范围，导致预测误差累积。

Ross等\cite{Ross2011DAgger}严格证明了BC的期望代价上界与时间步$T$呈$O(T^2)$增长：
\begin{equation}
  J(\pi_\theta) \le J(\pi^*) + T^2 \epsilon
\end{equation}
其中$\epsilon = \max_{s \in d_{\pi^*}} \ell(\pi_\theta(s), \pi^*(s))$为单步最大损失。这一$O(T^2)$的增长速率意味着：即使单步误差很小（如$\epsilon = 0.01$），在$T=500$步的长轨迹中也可能累积到灾难性水平。

\begin{figure}[htbp]
\centering
\includegraphics[width=0.92\textwidth]{pictures/普通行为克隆.png}
\caption{行为克隆（BC）端到端训练流程：左侧由专家策略$\pi^*$在环境中采集观测--动作对构成数据集$\mathcal{D}$；中间将序列观测输入端到端神经网络$\pi_\theta$预测动作$\hat{a}_t$；右侧通过MSE损失$\mathcal{L} = \|a_t - \hat{a}_t\|^2$计算梯度并反向传播更新网络参数}
\label{fig:distribution_shift}
\end{figure}

如图~\ref{fig:distribution_shift}所示，训练数据覆盖的状态空间（蓝色）与部署时策略实际访问的状态空间（橙色）存在偏移。在不重叠区域，策略从未见过类似状态，输出质量没有保障。Codevilla等\cite{Codevilla2019ExploringLimits}系统探索了BC在自动驾驶中的局限性，进一步证实了这一现象的普遍性。

\subsection{DAgger：数据集聚合}

DAgger（Dataset Aggregation）\cite{Ross2011DAgger}通过迭代式数据集聚合系统缓解分布偏移。算法流程如下：

\begin{enumerate}
  \item 以初始BC策略$\pi_0$（或随机策略）为起点；
  \item 第$i$轮：以混合策略$\hat{\pi}_i = \beta_i \pi^* + (1-\beta_i) \pi_i$在环境中采集数据，其中$\beta_i$为专家混合比例；
  \item 由专家策略$\pi^*$为所有采集到的状态标注最优动作；
  \item 将新标注数据合并到训练集$\mathcal{D}_i = \mathcal{D}_{i-1} \cup \mathcal{D}_{\text{new}}$；
  \item 以$\mathcal{D}_i$重新训练得到$\pi_{i+1}$。
\end{enumerate}

DAgger的闭环数据聚合直观流程如图~\ref{fig:dagger_loop}所示，迭代式数据聚合全流程示意见图~\ref{fig:dagger_detail}。

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
  >=Stealth,
  node distance=0.8cm and 1.0cm,
  block/.style={draw, rounded corners=3pt, minimum width=2.2cm, minimum height=0.9cm, align=center, font=\small},
  arrow/.style={->, thick, color=black!70},
  data/.style={draw, rounded corners=3pt, fill=yellow!15, minimum width=2.2cm, minimum height=0.9cm, align=center, font=\small},
]
\node[block, fill=orange!15] (policy) {当前策略 $\pi_i$};
\node[block, fill=blue!10, right=1.5cm of policy] (rollout) {在线采集\\闭环数据};
\node[block, fill=green!10, below=of rollout] (expert) {专家标注\\$a^* = \pi^*(o)$};
\node[data, below=of policy] (dataset) {聚合数据集\\$\mathcal{D}_i$};
\node[block, fill=orange!10, left=1.5cm of dataset] (retrain) {重新训练\\$\pi_{i+1}$};

\draw[arrow] (policy) -- (rollout);
\draw[arrow] (rollout) -- (expert);
\draw[arrow] (expert) -- (dataset);
\draw[arrow] (dataset) -- (retrain);
\draw[arrow] (retrain) |- (policy);

\node[font=\scriptsize, color=gray] at (3.0, -2.5) {迭代 $i = 1, 2, \ldots, N$};
\end{tikzpicture}
\caption{DAgger数据聚合闭环流程}
\label{fig:dagger_loop}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.92\textwidth]{pictures/DAgger.png}
\caption{DAgger迭代式数据聚合全流程示意：上层为环境交互阶段，混合策略$\hat{\pi}_i = \beta_i \pi^* + (1-\beta_i)\pi_i$在环境中采集轨迹并由专家$\pi^*$修正标注；中层为数据聚合阶段，新采集数据$\mathcal{D}_{\text{new}}$与历史数据集$\mathcal{D}_i$合并；下层为训练更新阶段，以聚合数据集重训策略$\pi_{i+1}$。右侧对比图展示BC误差$O(T^2)$增长与DAgger误差$O(1)$收敛的理论差异}
\label{fig:dagger_detail}
\end{figure}

DAgger的理论分析表明，经过$N$轮迭代后策略的期望损失上界降至$O(1)$：
\begin{equation}
  J(\hat{\pi}_N) \le J(\pi^*) + O\left(\frac{1}{N}\right)T \epsilon_N
\end{equation}
其中$\epsilon_N$为第$N$轮最优策略在聚合分布上的损失。这意味着DAgger理论上能够消除$O(T^2)$的累积效应。

后续变体包括SafeDAgger\cite{Zhang2016QueryDAgger}（基于安全代理判断是否查询专家）、HG-DAgger\cite{Kelly2019HG_DAgger}（人机交互模式）等。本文采用标准DAgger框架以保持方法简洁性，具体工程实现细节见第3章。

\subsection{DAgger的工程化实现口径}

DAgger的理论优美，但工程实现中有多个容易出错的细节需要明确：

\begin{itemize}
  \item $\beta$混合的实现方式：本文采用"状态级混合"，即在每个控制步以概率$\beta$执行专家动作、以概率$1-\beta$执行学生动作。另一种实现方式是"轨迹级混合"（前$\beta$比例的轨迹用专家采集），但状态级混合能更好地覆盖学生策略的错误状态；
  \item 专家标注的时机：无论实际执行的是专家还是学生动作，所有状态都由专家标注。这保证了每个状态都有正确的监督信号；
  \item 数据不平衡处理：随着DAgger轮次增加，新增数据量远小于初始BC数据。本文的处理方式是全量重训而非增量微调，以避免遗忘效应；
  \item 采集策略的选择：每轮新增数据偏重高速段（$\SI{9}{m/s}$、$\SI{12}{m/s}$各6条轨迹），因为这是BC基线最脆弱的区域。
\end{itemize}


\section{视觉表征：CNN与ViT}

\subsection{卷积神经网络}

卷积神经网络（CNN）\cite{Lecun1998CNN}通过局部感受野、权重共享与层级特征提取建立了视觉表征的基础范式。ResNet\cite{He2016ResNet}引入残差连接使训练更深的网络成为可能；VGG\cite{Simonyan2015VGG}以统一小卷积核展示了深度的重要性。在早期端到端避障工作中，CNN是默认的视觉编码器选择\cite{Loquercio2018DroNet,Sadeghi2017CAD2RL}。然而，CNN在全局结构关系建模方面受限于感受野大小：即使通过多层堆叠扩大理论感受野，实际有效感受野仍远小于输入尺寸\cite{Lecun1998CNN}。对于避障任务，这意味着CNN可能难以捕捉远距离障碍之间的空间关系。

\subsection{视觉Transformer（ViT）}

Dosovitskiy等提出的Vision Transformer（ViT）\cite{Dosovitskiy2020ViT}将Transformer\cite{Vaswani2017Transformer}范式引入图像识别：将图像划分为固定大小的patch token，经线性映射后输入标准Transformer编码器。如图~\ref{fig:vit_patch}所示，ViT通过自注意力机制建模任意patch对之间的全局依赖，突破了CNN的感受野限制。

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
  >=Stealth,
  node distance=0.4cm,
]
% 输入图像
\node[draw, fill=blue!5, minimum width=2.4cm, minimum height=1.6cm] (img) at (0, 0) {};
% 网格线
\draw[gray, thin] (-0.8, -0.8) grid[step=0.4] (1.2, 0.8);
\node[font=\scriptsize] at (0, -1.2) {输入图像 ($H{\times}W$)};

% 箭头
\draw[->, thick] (1.6, 0) -- (2.4, 0);

% Patch tokens
\foreach \i in {0,...,5} {
  \node[draw, fill=orange!20, minimum width=0.35cm, minimum height=0.35cm] at (2.8+\i*0.45, 0.4) {};
}
\node[font=\scriptsize] at (4.0, -0.1) {Patch Tokens};
\node[font=\scriptsize, color=gray] at (4.0, -0.5) {$N = HW/P^2$};

% 箭头
\draw[->, thick] (5.6, 0.2) -- (6.4, 0.2);
\node[font=\scriptsize] at (6.0, -0.2) {线性嵌入};

% Transformer编码器
\node[draw, fill=orange!10, rounded corners=3pt, minimum width=2.2cm, minimum height=1.6cm, align=center, font=\small] at (8.0, 0.2) {Transformer\\编码器\\(自注意力)};

% 箭头
\draw[->, thick] (9.3, 0.2) -- (10.0, 0.2);

% 输出
\node[draw, fill=green!10, rounded corners=3pt, minimum width=1.2cm, minimum height=0.8cm, align=center, font=\small] at (10.8, 0.2) {特征\\向量};
\end{tikzpicture}
\caption{ViT的patch token化与Transformer编码流程示意}
\label{fig:vit_patch}
\end{figure}

在四旋翼避障方向，Xing等\cite{Xing2024VisionBackbone}系统比较了多种视觉backbone，指出ViT在高速与泛化条件下具备明显优势。后续DeiT\cite{Touvron2021DeiT}通过知识蒸馏在无需大规模预训练数据的条件下提升ViT的训练效率；Swin Transformer\cite{Liu2021SwinTransformer}通过分层窗口注意力降低计算复杂度并引入多尺度特征；MAE\cite{He2022MAE}与BEiT\cite{Bao2022BEiT}进一步探索了大规模自监督预训练方法。

\subsection{轻量化ViT的设计维度}

在端到端控制场景中，视觉编码器的设计需要在表征能力与推理效率之间取得平衡。影响ViT效率的核心参数是patch数量$N$：自注意力的计算复杂度为$O(N^2 \cdot d)$，其中$d$为嵌入维度。表~\ref{tab:vit_complexity}展示了不同分辨率与patch size组合下的token数量及其对推理效率的影响。

\begin{table}[htbp]
\centering
\caption{不同输入分辨率与Patch Size下的Token数量与注意力复杂度}
\label{tab:vit_complexity}
\zihao{5}
\begin{tabular}{ccccc}
\toprule
\textbf{输入分辨率} & \textbf{Patch Size} & \textbf{Token数} $N$ & \textbf{注意力复杂度} $O(N^2)$ & \textbf{相对复杂度} \\
\midrule
$60 \times 90$ & $16 \times 16$ & 21 & $441$ & $1.0\times$ \\
$60 \times 90$ & $8 \times 8$ & 84 & $7{,}056$ & $16\times$ \\
$120 \times 180$ & $16 \times 16$ & 84 & $7{,}056$ & $16\times$ \\
$120 \times 180$ & $8 \times 8$ & 337 & $113{,}569$ & $257\times$ \\
$224 \times 224$ & $16 \times 16$ & 196 & $38{,}416$ & $87\times$ \\
\bottomrule
\end{tabular}
\end{table}

本文选择$60 \times 90$输入分辨率配合两阶段卷积嵌入（而非标准patch嵌入），使第一阶段token数为$16 \times 24 = 384$，第二阶段下采样至$8 \times 12 = 96$，在保留空间细节的同时控制计算量。这一设计使得ViT编码器在NVIDIA RTX 4060 GPU上的推理延迟可控制在$\SI{5}{ms}$以内。第3章将给出各模块的详细耗时分析。


\section{时序建模：RNN/LSTM与SSM}

\subsection{LSTM的流式优势与局限}

循环神经网络（RNN）\cite{Elman1990RNN}及其变体LSTM\cite{Hochreiter1997LSTM}通过门控机制选择性地保留与更新记忆状态，是端到端控制中最早用于时序聚合的模型。LSTM的单步递推形式为：
\begin{align}
  \mathbf{f}_t &= \sigma(\mathbf{W}_f [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_f) &\text{（遗忘门）} \\
  \mathbf{i}_t &= \sigma(\mathbf{W}_i [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_i) &\text{（输入门）} \\
  \mathbf{c}_t &= \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tanh(\mathbf{W}_c [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_c) &\text{（记忆更新）} \\
  \mathbf{o}_t &= \sigma(\mathbf{W}_o [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_o) &\text{（输出门）} \\
  \mathbf{h}_t &= \mathbf{o}_t \odot \tanh(\mathbf{c}_t) &\text{（隐状态）}
\end{align}

LSTM的优势在于天然支持流式递推推理：每步仅需输入当前观测并更新固定大小的隐状态$(\mathbf{h}_t, \mathbf{c}_t)$。然而，LSTM面临明确的局限：（1）长期依赖建模受限——虽然门控缓解了梯度消失，但实际中有效记忆范围通常在50--200步\cite{Hochreiter1997LSTM}；（2）训练效率低——序列依赖性阻碍并行化，训练速度远慢于Transformer；（3）部署状态管理敏感——隐状态$(\mathbf{h}_t, \mathbf{c}_t)$的管理同样面临第4章所讨论的一致性问题。

\subsection{结构化状态空间模型（S4）}

结构化状态空间模型（SSM）基于连续时间线性状态空间方程进行序列建模\cite{Gu2022S4}：
\begin{equation}
  \mathbf{h}'(t) = \mathbf{A}\mathbf{h}(t) + \mathbf{B}\mathbf{x}(t), \quad \mathbf{y}(t) = \mathbf{C}\mathbf{h}(t) + \mathbf{D}\mathbf{x}(t)
  \label{eq:ssm}
\end{equation}
其中$\mathbf{h}(t) \in \mathbb{R}^{d_{\text{state}}}$为连续时间隐状态，$\mathbf{A} \in \mathbb{R}^{d_{\text{state}} \times d_{\text{state}}}$为状态转移矩阵，$\mathbf{B} \in \mathbb{R}^{d_{\text{state}} \times 1}$为输入投影矩阵，$\mathbf{C} \in \mathbb{R}^{1 \times d_{\text{state}}}$为输出投影矩阵。

S4\cite{Gu2022S4}通过对矩阵$\mathbf{A}$的HiPPO\cite{Gu2020HiPPO}结构化初始化实现了长序列上的高效训练。后续工作S5\cite{Smith2023S5}进一步简化了实现；DSS\cite{Gu2022DSS}探索了对角参数化方案。

\subsection{从连续到离散的零阶保持（ZOH）推导}

在实际的离散时间系统中，需要将连续时间SSM离散化。零阶保持（Zero-Order Hold, ZOH）是最常用的离散化方法，其假设输入在采样间隔$\Delta$内保持恒定。

给定连续时间方程$\mathbf{h}'(t) = \mathbf{A}\mathbf{h}(t) + \mathbf{B}\mathbf{x}(t)$，在$[t_k, t_{k+1})$区间内（$t_{k+1} = t_k + \Delta$），输入$\mathbf{x}(t) = \mathbf{x}_k$为常数。该常微分方程的解析解为：
\begin{equation}
  \mathbf{h}(t_{k+1}) = e^{\mathbf{A}\Delta} \mathbf{h}(t_k) + \left(\int_0^{\Delta} e^{\mathbf{A}\tau} d\tau \right) \mathbf{B} \mathbf{x}_k
\end{equation}

令$\bar{\mathbf{A}} = e^{\mathbf{A}\Delta}$为离散化后的状态转移矩阵，$\bar{\mathbf{B}} = \left(\int_0^{\Delta} e^{\mathbf{A}\tau} d\tau \right) \mathbf{B} = (\mathbf{A})^{-1}(e^{\mathbf{A}\Delta} - \mathbf{I})\mathbf{B}$为离散化后的输入矩阵，则离散化后的递推方程为：
\begin{equation}
  \mathbf{h}_k = \bar{\mathbf{A}} \mathbf{h}_{k-1} + \bar{\mathbf{B}} \mathbf{x}_k, \quad \mathbf{y}_k = \mathbf{C} \mathbf{h}_k
  \label{eq:ssm_discrete}
\end{equation}

这一形式揭示了SSM与LSTM的结构相似性：两者都是"状态 $\times$ 转移矩阵 + 输入 $\times$ 投影矩阵"的线性递推。然而，SSM的关键优势在于：（1）矩阵$\bar{\mathbf{A}}$可以通过对角化实现高效并行扫描\cite{Gu2022S4}；（2）连续时间参数化为离散化步长$\Delta$提供了自适应调节的空间。

\subsection{Mamba的选择性机制}

在S4的基础上，Gu与Dao提出Mamba\cite{Gu2023Mamba}，核心创新是选择性机制：使参数$\mathbf{B}_t, \mathbf{C}_t$与离散化步长$\Delta_t$依赖于输入内容，增强模型对不同输入的自适应选择能力：
\begin{equation}
  \Delta_t = \text{softplus}(\mathbf{W}_\Delta \mathbf{x}_t + \mathbf{b}_\Delta), \quad
  \mathbf{B}_t = \mathbf{W}_B \mathbf{x}_t, \quad
  \mathbf{C}_t = \mathbf{W}_C \mathbf{x}_t
  \label{eq:mamba_selective}
\end{equation}

选择性机制的直觉理解如下：

\begin{itemize}
  \item $\Delta_t$控制"记忆保留时间"：当$\Delta_t$较大时，$\bar{\mathbf{A}}_t = e^{\mathbf{A}\Delta_t}$趋近于零矩阵，历史信息衰减加速，模型更倾向于"忘记过去、关注当前"；当$\Delta_t$较小时，历史信息保留更多；
  \item $\mathbf{B}_t$控制"什么信息写入"记忆：输入相关的$\mathbf{B}_t$使模型能够选择性地将当前输入的特定维度写入隐状态；
  \item $\mathbf{C}_t$控制"从记忆中读取什么"信息：输入相关的$\mathbf{C}_t$使模型能够根据当前上下文从隐状态中提取最相关的信息。
\end{itemize}

对于控制任务而言，这种选择性机制具有天然的适配性：当障碍接近时，$\Delta_t$可以自适应地增大以提高对最新观测的敏感度（快速反应）；当飞行路径平坦时，$\Delta_t$减小以更好地利用历史信息进行平滑预测。

SSM/Mamba的三层架构总览与选择性机制直觉如图~\ref{fig:mamba_overview}所示。

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{pictures/Mamba.png}
\caption{SSM/Mamba的三层架构总览。上层：连续时间状态空间方程$\mathbf{h}'(t) = \mathbf{A}\mathbf{h}(t) + \mathbf{B}\mathbf{x}(t)$，由矩阵$\mathbf{A}$驱动状态转移，$\mathbf{B}$控制输入投影，$\mathbf{C}$控制输出读取；中层：零阶保持（ZOH）离散化，将连续参数转化为离散递推$\bar{\mathbf{A}} = e^{\mathbf{A}\Delta}$；下层：Mamba选择性机制，$\Delta_t$、$\mathbf{B}_t$、$\mathbf{C}_t$均依赖输入$\mathbf{x}_t$动态生成。右侧展示了自适应控制直觉与LSTM门控结构的类比}
\label{fig:mamba_overview}
\end{figure}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
  >=Stealth,
  block/.style={draw, rounded corners=3pt, minimum width=1.6cm, minimum height=0.8cm, align=center, font=\small},
  arrow/.style={->, thick, color=black!70},
  state/.style={draw, circle, minimum size=0.8cm, font=\small},
]
% 时间步 t-1
\node[block, fill=blue!10] (x0) at (0, 0) {输入 $t{-}1$};
\node[state, fill=orange!15] (h0) at (0, 1.5) {$\mathbf{h}_{t-1}$};
\node[block, fill=green!10] (y0) at (0, 3.0) {输出 $t{-}1$};
\draw[arrow] (x0) -- node[right, font=\scriptsize] {$\bar{\mathbf{B}}_{t-1}$} (h0);
\draw[arrow] (h0) -- node[right, font=\scriptsize] {$\mathbf{C}_{t-1}$} (y0);
% 时间步 t
\node[block, fill=blue!10] (x1) at (3.5, 0) {输入 $t$};
\node[state, fill=orange!15] (h1) at (3.5, 1.5) {$\mathbf{h}_{t}$};
\node[block, fill=green!10] (y1) at (3.5, 3.0) {输出 $t$};
\draw[arrow] (x1) -- node[right, font=\scriptsize] {$\bar{\mathbf{B}}_{t}$} (h1);
\draw[arrow] (h1) -- node[right, font=\scriptsize] {$\mathbf{C}_{t}$} (y1);
% 时间步 t+1
\node[block, fill=blue!10] (x2) at (7.0, 0) {输入 $t{+}1$};
\node[state, fill=orange!15] (h2) at (7.0, 1.5) {$\mathbf{h}_{t+1}$};
\node[block, fill=green!10] (y2) at (7.0, 3.0) {输出 $t{+}1$};
\draw[arrow] (x2) -- node[right, font=\scriptsize] {$\bar{\mathbf{B}}_{t+1}$} (h2);
\draw[arrow] (h2) -- node[right, font=\scriptsize] {$\mathbf{C}_{t+1}$} (y2);
% 状态传播
\draw[arrow, red!60, very thick] (h0) -- node[above, font=\scriptsize, color=red!60] {$\bar{\mathbf{A}}$} (h1);
\draw[arrow, red!60, very thick] (h1) -- node[above, font=\scriptsize, color=red!60] {$\bar{\mathbf{A}}$} (h2);

\node[font=\scriptsize, color=red!60] at (3.5, -0.8) {$\mathbf{h}_t = \bar{\mathbf{A}}\mathbf{h}_{t-1} + \bar{\mathbf{B}}_t\mathbf{x}_t$, \quad $\mathbf{y}_t = \mathbf{C}_t\mathbf{h}_t$};
\end{tikzpicture}
\caption{SSM/Mamba离散化后的状态更新框图。下标$t$表示$\bar{\mathbf{B}}_t$、$\mathbf{C}_t$随输入动态变化（选择性机制）。}
\label{fig:ssm_block}
\end{figure}

离散化后的递推形式（图~\ref{fig:ssm_block}）与LSTM具有相似的递推结构。Mamba-2\cite{Dao2024Mamba2}进一步揭示了SSM与注意力机制之间的对偶性，统一了两种范式的理论基础。

\subsection{SSM对控制任务的意义}

表~\ref{tab:ssm_control_map}从四个维度分析了SSM特性与控制任务需求之间的映射关系。

\begin{table}[htbp]
\centering
\caption{SSM特性与高速避障控制需求的映射}
\label{tab:ssm_control_map}
\zihao{5}
\begin{tabular}{p{2.5cm}p{4.5cm}p{5.0cm}}
\toprule
\textbf{SSM特性} & \textbf{技术含义} & \textbf{对控制任务的价值} \\
\midrule
线性递推 & $O(n)$复杂度，流式推理友好 & 满足实时控制频率约束 \\
选择性机制 & $\Delta_t, \mathbf{B}_t, \mathbf{C}_t$依赖输入 & 自适应调节观测噪声抑制强度 \\
固定大小隐状态 & 状态维度不随序列长度增长 & 内存占用可预测，适合嵌入式部署 \\
连续时间参数化 & $\bar{\mathbf{A}} = e^{\mathbf{A}\Delta}$ & 对不等间距控制步自然适配 \\
\bottomrule
\end{tabular}
\end{table}

如图~\ref{fig:attn_vs_ssm}所示，自注意力机制的$O(n^2)$复杂度与SSM的$O(n)$复杂度形成鲜明对比，这一效率优势对实时控制至关重要。

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
\begin{axis}[
  width=7.5cm, height=4.5cm,
  xlabel={序列长度 $n$},
  ylabel={相对计算量},
  xmin=0, xmax=100,
  ymin=0, ymax=10000,
  xtick={0,25,50,75,100},
  legend pos=north west,
  legend style={font=\small},
  grid=major,
  grid style={gray!20},
]
\addplot[domain=0:100, samples=50, thick, color=red!70, dashed] {x^2};
\addlegendentry{Attention $O(n^2)$}
\addplot[domain=0:100, samples=50, thick, color=blue!70] {x*30};
\addlegendentry{SSM $O(n)$}
\addplot[domain=0:100, samples=50, thick, color=green!60!black, dashdotted] {x*x*0.3 + x*10};
\addlegendentry{LSTM $O(n \cdot d^2)$}
\end{axis}
\end{tikzpicture}
\caption{Attention、SSM与LSTM的序列长度--计算量关系对比（示意）}
\label{fig:attn_vs_ssm}
\end{figure}


\section{MambaVision：混合Mamba-Transformer视觉骨干}

MambaVision\cite{Hatamizadeh2025MambaVisionCVPR}是面向视觉应用的混合Mamba-Transformer backbone。其核心设计包括：（1）对Mamba模块进行面向视觉特征建模的重新设计，将一维序列建模扩展到二维空间特征；（2）通过系统消融验证在不同阶段融合自注意力块与Mamba块的最优配比——浅层使用Mamba捕捉局部空间特征，深层使用自注意力建模全局关系；（3）在ImageNet分类、COCO检测与ADE20K分割等基准上展示优于纯ViT与纯Mamba方案的效率--精度权衡。

Vision Mamba（Vim）\cite{Zhu2024VisionMamba}则采用另一条路线：以双向状态空间模型替代ViT中的自注意力机制，实现纯SSM视觉编码。两种方案分别代表了"混合"与"纯SSM"两条视觉骨干设计路线，本文在第5章对此进行控制变量实验。


\section{仿真平台与数据来源}

\subsection{Flightmare仿真平台}

本文所有实验在Flightmare高保真仿真平台\cite{Song2021Flightmare}中完成。Flightmare的设计强调物理引擎与渲染引擎的解耦：物理仿真可以在不启动渲染的情况下以极高速率运行（用于大规模数据生成），也可以启动渲染以支持视觉观测生成。与AirSim\cite{Shah2018AirSim}和RotorS\cite{Furrer2016RotorS}等其他无人机仿真器相比，Flightmare以"物理--渲染解耦"的设计在数据生成效率上具有显著优势。Agilicious\cite{Foehn2022Agilicious}提供了开放软硬件一体化平台，覆盖从MPC到神经网络控制的系统化验证。

\subsection{评测环境}

评测环境包含两类障碍分布，如表~\ref{tab:env_config}所示：

\begin{table}[htbp]
\centering
\caption{评测环境配置}
\label{tab:env_config}
\zihao{5}
\begin{tabular}{p{2.5cm}p{2.5cm}p{6.0cm}}
\toprule
\textbf{环境名称} & \textbf{分布类型} & \textbf{障碍特征} \\
\midrule
Spheres & 同分布（ID） & 三维空间中随机分布的球体障碍，训练数据在该环境中生成。障碍半径与密度参数化控制。 \\
Trees & 分布外（OOD） & 树状结构障碍：细长圆柱模拟树干 + 半球冠层。策略从未在该环境中训练，测试零样本迁移能力。 \\
\bottomrule
\end{tabular}
\end{table}

设置两类环境的目的是分别评估策略的"训练分布内性能"和"分布外泛化能力"。Trees环境的独特挑战在于：（1）树干在低分辨率深度图中仅占少数像素，容易遗漏；（2）冠层的形状与训练分布差异大，可能导致距离估计偏差。

两类评测环境的实拍截图如图~\ref{fig:env_screenshots}所示。

\begin{figure}[htbp]
\centering
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{pictures/Spheres.png}
\centerline{(a) Spheres环境（同分布）}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{pictures/Trees.png}
\centerline{(b) Trees环境（分布外）}
\end{minipage}
\caption{Flightmare仿真平台中两类评测环境的实拍截图。(a) Spheres环境：三维空间中随机分布不同半径的球体障碍，训练数据在该环境中生成；(b) Trees环境：由树干与冠层构成的自然场景，策略从未在此环境中训练，用于测试零样本迁移泛化能力}
\label{fig:env_screenshots}
\end{figure}

\subsection{特权信息专家策略}

训练数据由特权信息专家策略在Spheres环境中生成。与端到端策略不同，专家策略在每个控制步可访问完整环境信息（无人机精确位置/速度、所有障碍物的位置/几何参数），通过候选速度采样与碰撞检测生成高质量速度指令。算法~\ref{alg:expert}给出专家策略的伪代码。

\begin{algorithm}[htbp]
\caption{特权信息专家策略}
\label{alg:expert}
\begin{algorithmic}[1]
\Require 无人机状态 $(\mathbf{p}_t, \mathbf{v}_t, q_t)$，障碍集合 $\mathcal{O}_{\text{env}}$，目标速度 $v^{\text{target}}$
\Ensure 专家速度指令 $\mathbf{v}_t^*$
\State \textbf{// 候选速度采样}
\State $\mathcal{V}_{\text{cand}} \leftarrow$ 在目标速度方向锥体内均匀采样 $K$ 个候选方向
\For{每个候选方向 $\hat{\mathbf{d}}_k \in \mathcal{V}_{\text{cand}}$}
  \State 构造候选速度 $\mathbf{v}_k = v^{\text{target}} \cdot \hat{\mathbf{d}}_k$
  \State \textbf{// 碰撞检测与安全裕度评估}
  \State $c_k \leftarrow \min_{\mathbf{o} \in \mathcal{O}_{\text{env}}} \text{clearance}(\mathbf{p}_t + \mathbf{v}_k \cdot \Delta t_{\text{lookahead}}, \mathbf{o})$
  \State \textbf{// 代价函数：安全性 + 目标方向对齐 + 平滑性}
  \State $\text{cost}_k \leftarrow -\alpha_1 c_k + \alpha_2 \|\hat{\mathbf{d}}_k - \hat{\mathbf{x}}\|_2 + \alpha_3 \|\mathbf{v}_k - \mathbf{v}_{t-1}^*\|_2$
\EndFor
\State $k^* \leftarrow \arg\min_k \text{cost}_k$
\State \Return $\mathbf{v}_t^* = \mathbf{v}_{k^*}$
\end{algorithmic}
\end{algorithm}

表~\ref{tab:expert_params}给出专家策略的超参数配置。

\begin{table}[htbp]
\centering
\caption{特权信息专家策略超参数}
\label{tab:expert_params}
\zihao{5}
\begin{tabular}{lcc}
\toprule
\textbf{参数} & \textbf{符号} & \textbf{数值} \\
\midrule
候选方向采样数 & $K$ & 128 \\
前视时间 & $\Delta t_{\text{lookahead}}$ & $\SI{0.5}{s}$ \\
安全裕度权重 & $\alpha_1$ & 1.0 \\
方向对齐权重 & $\alpha_2$ & 0.3 \\
平滑性权重 & $\alpha_3$ & 0.1 \\
采样锥体半角 & -- & $60^\circ$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{数据采集管线}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
  >=Stealth,
  node distance=0.6cm and 0.8cm,
  block/.style={draw, rounded corners=3pt, minimum width=2.4cm, minimum height=0.9cm, align=center, font=\small},
  arrow/.style={->, thick, color=black!70},
  data/.style={draw, rounded corners=3pt, fill=yellow!10, minimum width=2.4cm, minimum height=0.9cm, align=center, font=\small},
]
\node[block, fill=blue!10] (scene) {场景随机化\\(Spheres环境)};
\node[block, fill=green!10, right=of scene] (expert) {特权信息\\专家策略};
\node[block, fill=orange!10, right=of expert] (sim) {Flightmare\\闭环仿真};
\node[data, right=of sim] (traj) {轨迹数据\\$(D_t, s_t, a_t^*)$};
\node[data, below=0.8cm of traj] (dataset) {训练数据集\\(585条轨迹)};

\draw[arrow] (scene) -- (expert);
\draw[arrow] (expert) -- (sim);
\draw[arrow] (sim) -- (traj);
\draw[arrow] (traj) -- (dataset);

\node[font=\scriptsize, color=gray] at (5.5, -2.0) {专家可访问完整环境信息（位置、速度、障碍几何）};
\end{tikzpicture}
\caption{基于Flightmare与特权信息专家的数据采集管线}
\label{fig:data_pipeline}
\end{figure}

如图~\ref{fig:data_pipeline}所示，训练数据由特权信息专家在Spheres环境中生成。每条轨迹包含深度图像$D_t$、无人机状态$s_t$与专家速度指令$a_t^*$的时间序列。训练数据集包含约585条专家轨迹，覆盖5个速度档（$\SI{3}{m/s}$--$\SI{12}{m/s}$），轨迹长度在200--800步之间。注意，Trees环境不参与任何训练数据的生成，仅用于零样本OOD评测。


\section{评测协议与指标}

本节固定全篇统一的评测协议与指标定义。后续各章实验直接引用本节表格与定义，不再重复。

\subsection{统一评测协议}

统一评测协议如表~\ref{tab:eval_protocol_unified}所示。

\begin{table}[htbp]
\centering
\caption{统一评测协议}
\label{tab:eval_protocol_unified}
\zihao{5}
\begin{tabular}{lc}
\toprule
\textbf{参数} & \textbf{设置} \\
\midrule
目标速度档位 & 3, 5, 7, 9, 12 m/s \\
每档试验次数 & 10次 \\
回合终止距离 & 沿$X$轴 58--60 m \\
超时限制 & $\tau_{\max} = \SI{40}{s}$ \\
碰撞处理 & 不终止回合，持续记录 \\
状态管理 & KeepState（回合级重置） \\
测试环境 & Spheres（ID） + Trees（OOD） \\
随机种子 & 固定（PyTorch + NumPy + CUDA确定性） \\
硬件配置 & NVIDIA RTX 4060 GPU (8GB) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{指标定义}

表~\ref{tab:metric_def}给出了本文使用的所有评测指标的严格定义。

\begin{table}[htbp]
\centering
\caption{评测指标定义与计算口径}
\label{tab:metric_def}
\zihao{5}
\begin{tabular}{p{2.5cm}p{5.5cm}p{2.5cm}p{2.0cm}}
\toprule
\textbf{指标名称} & \textbf{定义} & \textbf{单位} & \textbf{统计方式} \\
\midrule
全程碰撞率 (Collision Rate) & $\sum_{t=1}^{T}\mathbb{1}[\text{collision}_t=1] / T$ & \% & 10次均值$\pm$std \\
碰撞事件次数 (Collision Count) & 碰撞标志上升沿计数 & 次/回合 & 10次均值$\pm$std \\
成功率 (Success Rate) & 超时限内到达终点的回合比例 & \% & 10次比例 \\
指令抖动 (Command Jerk) & $\|\mathbf{v}_t - \mathbf{v}_{t-1}\|_2$ 回合内均值 & m/s & 10次均值$\pm$std \\
推理时间 & 单步模型前向推理耗时 & ms & 中位数 \\
横向漂移 (Mean Y Drift) & $\frac{1}{T}\sum_{t=1}^{T}|y_t|$ & m & 10次均值 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{指标计算伪代码}

为确保评测指标的计算可复现，本节给出关键指标的伪代码实现。

碰撞事件次数的计算采用上升沿检测：
\begin{equation}
  \text{Collision Count} = \sum_{t=2}^{T} \mathbb{1}[\text{collision}_t = 1 \wedge \text{collision}_{t-1} = 0]
  \label{eq:collision_count_ch2}
\end{equation}

\begin{algorithm}[htbp]
\caption{碰撞事件次数计算（上升沿检测）}
\label{alg:collision_count}
\begin{algorithmic}[1]
\Require 碰撞标志序列 $\texttt{collision}[1..T] \in \{0, 1\}^T$
\Ensure 碰撞事件次数 $\texttt{count}$
\State $\texttt{count} \leftarrow 0$
\For{$t = 2$ \textbf{to} $T$}
  \If{$\texttt{collision}[t] = 1$ \textbf{and} $\texttt{collision}[t-1] = 0$}
    \State $\texttt{count} \leftarrow \texttt{count} + 1$ \Comment{检测到上升沿}
  \EndIf
\EndFor
\State \Return $\texttt{count}$
\end{algorithmic}
\end{algorithm}

如图~\ref{fig:collision_edge}所示，连续碰撞帧视为同一次碰撞事件，仅统计上升沿以避免重复计数。

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
  >=Stealth,
]
% 时间轴
\draw[->, thick] (0, 0) -- (12, 0) node[right, font=\small] {时间 $t$};
\draw[->, thick] (0, 0) -- (0, 1.8) node[above, font=\small] {碰撞标志};

% 碰撞信号
\draw[very thick, blue!70] (0, 0) -- (2, 0) -- (2, 1.2) -- (4, 1.2) -- (4, 0) -- (7, 0) -- (7, 1.2) -- (8.5, 1.2) -- (8.5, 0) -- (11, 0);

% 上升沿标记
\draw[->, red!70, very thick] (2, -0.5) -- (2, 0);
\node[font=\scriptsize, color=red!70] at (2, -0.8) {上升沿1};
\draw[->, red!70, very thick] (7, -0.5) -- (7, 0);
\node[font=\scriptsize, color=red!70] at (7, -0.8) {上升沿2};

% 标注
\node[font=\scriptsize, color=blue!70] at (3, 1.6) {碰撞事件1};
\node[font=\scriptsize, color=blue!70] at (7.75, 1.6) {碰撞事件2};

% Collision Count
\node[draw, rounded corners=2pt, fill=yellow!10, font=\small] at (6, -1.6) {Collision Count = 2（仅统计上升沿）};
\end{tikzpicture}
\caption{碰撞事件次数的上升沿检测计算示意}
\label{fig:collision_edge}
\end{figure}

\begin{algorithm}[htbp]
\caption{Command Jerk计算}
\label{alg:jerk_calc}
\begin{algorithmic}[1]
\Require 速度指令序列 $\mathbf{v}[1..T] \in \mathbb{R}^{T \times 3}$
\Ensure 平均Jerk $\bar{J}$
\State $\texttt{jerk\_sum} \leftarrow 0$
\For{$t = 2$ \textbf{to} $T$}
  \State $\texttt{jerk\_sum} \leftarrow \texttt{jerk\_sum} + \|\mathbf{v}[t] - \mathbf{v}[t-1]\|_2$
\EndFor
\State $\bar{J} \leftarrow \texttt{jerk\_sum} / (T - 1)$
\State \Return $\bar{J}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[htbp]
\caption{横向漂移（Mean Y Drift）计算}
\label{alg:drift_calc}
\begin{algorithmic}[1]
\Require 位置序列 $\mathbf{p}[1..T] \in \mathbb{R}^{T \times 3}$
\Ensure 平均横向漂移 $\bar{D}_y$
\State $\bar{D}_y \leftarrow \frac{1}{T} \sum_{t=1}^{T} |p_y[t]|$ \Comment{$p_y$为$Y$轴分量}
\State \Return $\bar{D}_y$
\end{algorithmic}
\end{algorithm}

\subsection{统计显著性与不确定性报告}

本文评测中每个配置进行10次独立试验（固定种子但不同初始位置），报告均值$\pm$标准差。采用这一方案而非更复杂的统计检验（如$t$-test或bootstrap置信区间）的原因在于：

\begin{enumerate}
  \item 样本量限制：每档仅10次试验，样本量不满足正态性假设的可靠性要求；
  \item 效应量显著：本文的主要对比（如KeepState vs ResetState的碰撞率差异为$0\%$对$90\%$）效应量远超统计噪声；
  \item 标准差的信息量：标准差直接反映策略行为的稳定性，是衡量工程部署可靠性的关键指标——高标准差意味着策略行为不可预测，即使均值尚可，工程上也不可接受。
\end{enumerate}

\subsection{评测可审计规范}

为确保实验结论的可复现性与可追溯性，本文建立以下评测可审计规范：
\begin{enumerate}
  \item 随机种子固定：所有实验固定随机种子（包括PyTorch、NumPy、CUDA确定性模式与环境初始化种子）；
  \item 环境参数记录：每次评测自动记录环境类型、障碍密度参数、目标速度档位与回合终止条件等关键配置；
  \item 状态重置时机：明确记录序列模型内部状态的重置时机（仅在回合边界），并通过运行时断言确保回合内状态的连续传播（详见第4章）；
  \item 版本号固化：记录策略网络权重文件的哈希值、代码版本号与依赖库版本；
  \item 控制周期分布：记录每次试验中所有控制步的$\Delta t$时间间隔分布，用于排除系统负载差异造成的混淆因素。
\end{enumerate}

上述规范贯穿本文所有实验，确保评测结论不受实现细节污染。


\section{相关工作综述}

\subsection{端到端视觉飞行控制}

端到端控制通常以"视觉到控制"为核心。早期工作DroNet\cite{Loquercio2018DroNet}将CNN映射用于城市导航；CAD2RL\cite{Sadeghi2017CAD2RL}与Deep Drone Racing\cite{Kaufmann2018DeepDroneRacing}验证了仿真到现实迁移的可能性；Gandhi等\cite{Gandhi2017CollisionDrone}通过碰撞数据自监督学习避障；Kaufmann等的Deep Drone Acrobatics\cite{Kaufmann2020DeepDroneAcrobatics}将端到端扩展到极限机动动作。Loquercio等\cite{Loquercio2021HighSpeedWild}使用特权信息专家+域随机化实现了野外高速飞行；Kaufmann等的Swift系统\cite{Kaufmann2023SwiftNature}在竞速对抗中超越人类冠军。Pan等\cite{Pan2018AgileAutonomous}在自主驾驶场景中验证了端到端深度模仿学习的敏捷性。Shah等\cite{Shah2023GNM}提出的通用导航模型进一步将端到端范式推广到跨机器人平台。上述工作共同构成了"仿真学习+特权专家+域随机化"的标准范式。

\subsection{模块化自主飞行}

模块化方案的代表性工作包括：ORB-SLAM2\cite{MurArtal2017ORBSLAM2}和ORB-SLAM3\cite{Campos2021ORBSLAM3}（稀疏特征SLAM）、VINS-Mono\cite{Qin2018VINSMONO}（视觉惯性估计）、LSD-SLAM\cite{Engel2014LSDSLAM}（大尺度直接SLAM）、RRT*\cite{Karaman2011SamplingOptimal}（渐近最优采样规划）、FASTER\cite{Faust2018FASTER}（安全回退轨迹）。SLAM方面，Cadena等\cite{Cadena2016SLAMSurvey}全面综述了从经典到鲁棒感知时代的SLAM发展历程。轨迹优化方面，Mellinger与Kumar\cite{Mellinger2011MinSnapTrajectory}的最小snap轨迹生成、Richter等\cite{Richter2016MinSnapPoly}的多项式轨迹规划奠定了理论基础；MPC方面，Kamel等\cite{Kamel2017NMPC}和Neunert等\cite{Neunert2016MPC_Quadrotor}分别将非线性MPC应用于四旋翼轨迹跟踪。

国内方面，高翔等\cite{Gao2019SLAMSurvey}从特征法与直接法两条路线出发，系统分析了视觉SLAM在不同场景下的精度--效率权衡，并指出语义信息融合是下一阶段的关键方向；张弓等\cite{Zhang2018VIOSLAM}将视觉惯性SLAM方法按前端跟踪策略与后端优化框架进行分类，明确了紧耦合方案在高动态平台上的鲁棒性优势；吴潇等\cite{Wu2022QuadSLAM}针对无人机平台的振动干扰、快速运动模糊与有限算力三个特殊挑战，对比分析了轻量级视觉SLAM方案的可部署性。在轨迹规划方面，Zhou等提出Fast-Planner\cite{Zhou2019FastPlanner}，采用运动学路径搜索与B样条轨迹优化的两阶段方案实现未知环境中的在线安全飞行；其后续工作EGO-Planner\cite{Zhou2021EGOPlanner}进一步消除了对欧几里得符号距离场（ESDF）的依赖，直接从障碍点云计算碰撞梯度，将规划计算量降低一个数量级。何承坤等\cite{He2021QuadTrajectory}从最小snap多项式、B样条、安全走廊三条技术路线出发，对比分析了四旋翼轨迹优化在实时性与安全性之间的设计折中；张涛等\cite{Zhang2020AutoPilotSurvey}将无人机自主飞行控制技术按感知、决策、执行三层架构进行归纳，指出多传感器融合与自适应控制的集成是高可靠自主飞行的核心难题；刘小雄等\cite{Liu2020QuadControl}从反应式避障、基于地图的全局规划、学习型策略三个层面对四旋翼自主导航技术进行分层分析，指出端到端学习方法在低延迟要求下具有潜在优势但可解释性不足。

这些方法在低中速结构化场景中可靠，但在高速密集障碍环境中面临延迟与误差传播的固有局限。

\subsection{安全性与部署侧约束}

Brunke等\cite{Brunke2022SafeLearningReview}系统总结了安全学习控制的主要路线。控制障碍函数（CBF）\cite{Ames2019CBFSurvey}为安全约束提供形式化工具；Cheng等\cite{Cheng2019RLwithCBF}将CBF嵌入强化学习；Wabersich等\cite{Wabersich2018MPSC}提出MPSC预测安全滤波；Fisac等\cite{Fisac2019SafeRL}建立了通用的安全学习框架。Garc\'{i}a与Fern\'{a}ndez\cite{GarciaPineda2015SafeRLSurvey}从更广泛的视角全面综述了安全强化学习。Dalal等\cite{Dalal2018SafeExploration}探索了连续动作空间中的安全探索方法。

在国内，雷志勇等\cite{Lei2020DRLAvoidance}将深度Q网络（DQN）与连续动作空间策略梯度方法应用于无人机避障任务，验证了稀疏激光雷达输入下DRL策略的实时决策能力；陈杰等\cite{Chen2023DRLDroneReview}从状态表征、奖励函数设计、训练稳定性三个维度对DRL在无人机导航中的应用进行分类评述，指出仿真到实际迁移（Sim-to-Real）的鲁棒性瓶颈是制约DRL落地的主要因素；严旭等\cite{Yan2021DRLObstacle}提出将深度图与惯性测量数据融合作为DRL状态输入的方案，在三维动态障碍场景中实现了优于传统势场法的避障成功率；李超等\cite{Li2022RLUAV}对比分析了基于值函数与基于策略梯度两类强化学习方法在无人机路径规划中的收敛速度与最终性能差异，发现近端策略优化（PPO）在连续动作空间中表现最为稳定；朱福利等\cite{Zhu2021DeepLearningUAV}从目标检测、语义分割、深度估计三个视觉子任务出发，分析了深度学习在无人机感知系统中的部署约束与加速策略，指出模型压缩与边缘推理是实现机载实时处理的关键技术。

\subsection{Sim-to-Real迁移}

Sim-to-Real迁移是端到端控制从仿真走向实际部署的关键桥梁。Tobin等\cite{Tobin2017DomainRandomization}提出域随机化方法，通过在仿真中随机化视觉属性来弥合仿真与现实的差距。Peng等\cite{Peng2018SimtoRealRL}进一步将动力学随机化引入机器人控制迁移。Molchanov等\cite{Molchanov2019SimRL}验证了低层鲁棒控制策略向多个四旋翼平台的迁移。Zhao等\cite{Zhao2020SimtoReal}对深度强化学习的Sim-to-Real迁移进行了全面综述。本文当前工作在Flightmare仿真中完成验证，Sim-to-Real迁移作为未来工作方向在第5章讨论。

\subsection{方法谱系总结}

表~\ref{tab:route_compare}从四个维度对主要技术路线进行横向对比。

\begin{table}[htbp]
\centering
\caption{高速端到端视觉避障相关技术路线对比}
\label{tab:route_compare}
\zihao{5}
\begin{tabular}{p{1.5cm}p{2.8cm}p{2.8cm}p{2.5cm}p{2.5cm}}
\toprule
\textbf{对比维度} & \textbf{路线A} & \textbf{路线B} & \textbf{A的优势} & \textbf{B的优势} \\
\midrule
系统范式 &
模块化（感知--规划--控制） &
端到端（视觉$\to$控制） &
可解释、可验证 &
低延迟、架构简洁 \\
\midrule
训练方法 &
行为克隆（BC） &
DAgger/强化学习 &
训练稳定、样本高效 &
闭环分布覆盖更好 \\
\midrule
时序建模 &
LSTM/RNN &
SSM（Mamba） &
工程成熟、流式支持 &
线性复杂度、选择性机制 \\
\midrule
视觉编码 &
ViT &
MambaVision &
全局注意力、强表征 &
效率更优、架构统一 \\
\bottomrule
\end{tabular}
\end{table}


\section{小结：设计需求}

综合本章的预备知识与相关工作分析，对后续创新章节提出以下设计需求：

\begin{itemize}
  \item 需要低延迟的时序建模能力，以支撑高速闭环控制（$\rightarrow$ 第3章：ViT+Mamba）；
  \item 需要闭环数据增强机制以缓解BC的分布偏移（$\rightarrow$ 第3章：DAgger）；
  \item 需要部署侧平滑约束以控制敏捷性带来的指令抖动（$\rightarrow$ 第3章：RACS）；
  \item 需要严格的流式部署一致性验证机制（$\rightarrow$ 第4章：状态生命周期管理）；
  \item 需要在安全/平滑/延迟/显存四维做统一对比，评估SSM视觉骨干的可行性（$\rightarrow$ 第5章：MambaVision）；
  \item 需要可复现的指标口径与评测可审计规范（$\rightarrow$ 本章表~\ref{tab:eval_protocol_unified}与表~\ref{tab:metric_def}）。
\end{itemize}
