\chapter{预备知识与相关工作}

本章旨在系统性地论述支撑本文核心创新点的背景知识，
并确立贯穿全篇的评测协议与指标定义。
本章遵循“最小必要性”原则对相关背景进行梳理：
仅聚焦于支撑后续研究及改进方案所需的理论基础，
以此建立统一的评测基准与实验口径。
后续章节的实验部分将直接沿用本章定义的指标体系，
以确保全文论述的连贯性与严谨性。

\section{四旋翼控制接口与任务抽象}

\subsection{坐标系与控制量定义}

本文采用东北天（ENU）右手坐标系作为世界坐标系。
如图~\ref{fig:coord_frame}所示，
无人机的位置与速度定义在世界坐标系下，
姿态以四元数$q = [w, x, y, z]$表示机体坐标系相对于世界坐标系的旋转。

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
  >=Stealth, scale=0.9,
  axis/.style={->, thick},
]
% 世界坐标系
\node[font=\small\bfseries, color=blue!70, anchor=east] at (-0.8, 3.5) {世界坐标系 (World)};
\draw[axis, blue!70] (0,0) -- (3.0,0) node[right, font=\small] {$X$ (前进方向)};
\draw[axis, blue!70] (0,0) -- (0,3.0) node[left, font=\small] {$Z$ (竖直向上)};
\draw[axis, blue!70] (0,0) -- (-1.2,-1.2) node[below left, font=\small] {$Y$ (侧向)};

% 无人机简化图
\node[draw, fill=gray!20, rounded corners=2pt, minimum width=1.2cm, minimum height=0.4cm] (drone) at (6.0, 1.5) {};
\node[font=\scriptsize] at (6.0, 1.0) {四旋翼};

% 机体坐标系
\node[font=\small\bfseries, color=red!70] at (6.0, 3.8) {机体坐标系 (Body)};
\draw[axis, red!70] (6.0,1.5) -- (7.5,1.5) node[right, font=\small] {$x_b$};
\draw[axis, red!70] (6.0,1.5) -- (6.0,3.0) node[left, font=\small] {$z_b$};
\draw[axis, red!70] (6.0,1.5) -- (5.2,0.7) node[below left, font=\small] {$y_b$};

% 速度指令
\draw[->, very thick, green!60!black, dashed] (6.0,1.5) -- (8.0,2.8) node[right, font=\small, color=green!60!black] {$\mathbf{v}_{\text{cmd}} = [v^x, v^y, v^z]$};

% 姿态四元数标注
\node[draw, rounded corners=2pt, fill=yellow!10, font=\scriptsize, inner sep=3pt] at (3.2, -0.5) {姿态: $q_t = [w, x, y, z]$};
\end{tikzpicture}
\caption{世界坐标系与机体坐标系定义，以及速度指令接口}
\label{fig:coord_frame}
\end{figure}

策略网络在每个控制周期输出世界坐标系下的三维线速度指令$\mathbf{v}_t = [v^x_t, v^y_t, v^z_t] \in \mathbb{R}^3$，
该指令由低层控制器（姿态环+电机混控）转化为电机转速执行。
控制频率由策略推理速度决定，
在本文硬件配置下可达毫秒级。
经典四旋翼建模与控制理论可参见Mahony等\cite{Mahony2012QuadrotorSurvey}的综述。

\subsection{任务形式化}

本文研究的高速视觉避障任务形式化为序列决策问题。
在每个控制周期$t$，
策略$\pi_\theta$根据观测$o_t$输出控制动作$a_t$，
形成闭环：
\begin{equation}
  \mathcal{M} = \langle \mathcal{O}, \mathcal{A}, \mathcal{T}, \mathcal{G}, \tau_{\max} \rangle
  \label{eq:task_tuple}
\end{equation}
其中$\mathcal{O}$为观测空间（深度图像$D_t \in \mathbb{R}^{60 \times 90}$与轻量状态$s_t = [q_t, \tilde{v}^{\text{target}}]$），
$\mathcal{A}$为动作空间（世界坐标系下的速度指令$\mathbf{v}_t \in \mathbb{R}^3$），
$\mathcal{T}$为由仿真器物理引擎决定的状态转移函数，
$\mathcal{G}$为回合终止条件集合，
$\tau_{\max} = \SI{40}{s}$为最大回合时长。

策略以序列历史为条件输出当前动作：
\begin{equation}
  a_t = \pi_\theta(o_{\le t}, s_{\le t}) = \pi_\theta(D_{\le t}, q_{\le t}, \tilde{v}^{\text{target}})
  \label{eq:policy}
\end{equation}

\subsection{控制回路与低层控制器假设}

本文的端到端策略工作在速度指令层级，
将低层控制器视为黑盒。
具体地，
我们对低层控制器做以下假设：

\begin{enumerate}
  \item 一阶响应近似：低层控制器对速度指令的跟踪可近似为带延迟的一阶系统，
    即$\dot{\mathbf{v}}_{\text{actual}} = \frac{1}{\tau_c}(\mathbf{v}_{\text{cmd}} - \mathbf{v}_{\text{actual}})$，
    其中$\tau_c$为控制器时间常数（$\tau_c \approx \SI{50}{ms}$--$\SI{100}{ms}$）；
     \item 速度饱和：实际速度受物理限制不超过最大可达速度$v_{\max}$（在本文仿真环境中$v_{\max} \approx \SI{15}{m/s}$）；
     \item 姿态稳定性：低层控制器能够在策略输出的速度指令范围内保持姿态稳定，
    不发生失稳翻转。
     \end{enumerate}

上述假设确定了策略网络的"控制权限边界"：策略不需要关心电机级细节，
只需输出合理范围内的速度指令。
这一假设在Flightmare仿真平台\cite{Song2021Flightmare}中由内置的PID/几何控制器\cite{Lee2010GeometricControl}保证。

\subsection{安全指标与任务完成条件}

本文采用"碰撞不终止回合"的评测设定，
即无人机在碰撞后继续飞行。
这一设定的统计学优势在于：（1）避免了碰撞终止导致的幸存者偏差（survivor bias）——若碰撞后立即终止，
则高碰撞率策略的后续轨迹被截断，
无法公平比较完整回合的统计特性；
（2）能够同时统计碰撞率与成功率两个互补指标；
（3）保留了碰撞事件的完整时间序列，
支持更细粒度的碰撞事件分析（如碰撞持续时间、间隔分布等）。

回合终止条件包括：（1）无人机沿$X$轴飞行距离达到$\SI{58}{m}$--$\SI{60}{m}$（成功）；
（2）飞行时长超过$\tau_{\max} = \SI{40}{s}$（超时，
通常意味着策略因频繁碰撞而无法正常前进）。


\section{模仿学习与分布偏移：BC与DAgger}

\subsection{行为克隆（BC）}

行为克隆（Behavioral Cloning, BC）是端到端控制中最常用的训练范式\cite{Pomerleau1989ALVINN}：以专家策略$\pi^*$生成的状态--动作对$\{(o_t, a_t^*)\}$为监督信号，
通过最小化策略输出与专家动作之间的损失进行离线学习：
\begin{equation}
  \mathcal{L}_{\text{BC}} = \mathbb{E}_{(o,a^*) \sim d_{\pi^*}} \left[ \ell(\pi_\theta(o), a^*) \right]
  \label{eq:bc_general}
\end{equation}
其中$d_{\pi^*}$为专家策略诱导的状态分布，
$\ell(\cdot, \cdot)$为损失函数（本文采用均方误差MSE）。

BC的优势在于训练稳定、样本效率高、实现简单。
在端到端控制文献中，
从Pomerleau的ALVINN\cite{Pomerleau1989ALVINN}到NVIDIA自动驾驶\cite{Bojarski2016EndToEndNVIDIA}再到Codevilla等的条件模仿学习\cite{Codevilla2018EndToEndDriving}，
BC一直是基础训练方法。
Osa等\cite{Osa2018ImitationSurvey}对模仿学习的算法视角进行了全面综述。

\subsection{分布偏移与误差累积}

BC的核心问题在于闭环分布偏移（covariate shift）\cite{Ross2011DAgger}：训练数据由专家策略诱导的状态分布$d_{\pi^*}$生成，
而部署时策略访问的状态分布$d_{\pi_\theta}$由学生策略自身诱导。
当学生策略在某些状态下产生微小偏差$\epsilon$时，
后续状态会偏离专家数据的覆盖范围，
导致预测误差累积。

Ross等\cite{Ross2011DAgger}严格证明了BC的期望代价上界与时间步$T$呈$O(T^2)$增长：
\begin{equation}
  J(\pi_\theta) \le J(\pi^*) + T^2 \epsilon
\end{equation}
其中$\epsilon = \max_{s \in d_{\pi^*}} \ell(\pi_\theta(s), \pi^*(s))$为单步最大损失。
这一$O(T^2)$的增长速率意味着：即使单步误差很小（如$\epsilon = 0.01$），
在$T=500$步的长轨迹中也可能累积到灾难性水平。

\begin{figure}[htbp]
\centering
\includegraphics[width=0.92\textwidth]{Image/图2-1_行为克隆端到端训练流程.png}
\caption{行为克隆（BC）端到端训练流程：左侧由专家策略$\pi^*$在环境中采集观测--动作对构成数据集$\mathcal{D}$；中间将序列观测输入端到端神经网络$\pi_\theta$预测动作$\hat{a}_t$；右侧通过MSE损失$\mathcal{L} = \|a_t - \hat{a}_t\|^2$计算梯度并反向传播更新网络参数}
\label{fig:distribution_shift}
\end{figure}

如图~\ref{fig:distribution_shift}所示，
训练数据覆盖的状态空间（蓝色）与部署时策略实际访问的状态空间（橙色）存在偏移。
在不重叠区域，
策略从未见过类似状态，
输出质量没有保障。
Codevilla等\cite{Codevilla2019ExploringLimits}系统探索了BC在自动驾驶中的局限性，
进一步证实了这一现象的普遍性。

\subsection{DAgger：数据集聚合}

DAgger（Dataset Aggregation）\cite{Ross2011DAgger} 的核心思想是通过“在线干预”与“数据回流”建立反馈闭环。
该算法不再局限于专家生成的静态演示，而是将当前学习到的策略部署于环境中进行“试错”，
强制智能体探索自身可能诱发的非最优状态空间。
通过请求专家对这些真实交互状态进行在线补标，算法能够有针对性地纠正策略在偏离轨迹后的行为，
从而在训练过程中实现对潜在误差轨迹的覆盖。
其具体的迭代流程如下：

\begin{enumerate}
    \item 以初始行为克隆策略 $\pi_0$（或随机策略）作为训练起点；
    \item 第 $i$ 轮迭代：在环境中部署混合策略 $\hat{\pi}_i = \beta_i \pi^* + (1-\beta_i) \pi_i$ 采集交互轨迹，
    其中 $\beta_i$ 用于平衡专家引导与策略自主探索的比例；
    \item 引入专家策略 $\pi^*$ 为当前采集到的所有实时状态标注最优动作标量；
    \item 将新获得的交互数据聚合至全局训练集 $\mathcal{D}_i = \mathcal{D}_{i-1} \cup \mathcal{D}_{\text{new}}$；
    \item 在聚合后的数据集 $\mathcal{D}_i$ 上通过监督学习进行策略迭代，得到更新后的 $\pi_{i+1}$。
\end{enumerate}

DAgger的闭环数据聚合直观流程如图~\ref{fig:dagger_loop}所示，
迭代式数据聚合全流程示意见图~\ref{fig:dagger_detail}。

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
  >=Stealth,
  node distance=0.8cm and 1.0cm,
  block/.style={draw, rounded corners=3pt, minimum width=2.2cm, minimum height=0.9cm, align=center, font=\small},
  arrow/.style={->, thick, color=black!70},
  data/.style={draw, rounded corners=3pt, fill=yellow!15, minimum width=2.2cm, minimum height=0.9cm, align=center, font=\small},
]
\node[block, fill=orange!15] (policy) {当前策略 $\pi_i$};
\node[block, fill=blue!10, right=1.5cm of policy] (rollout) {在线采集\\闭环数据};
\node[block, fill=green!10, below=of rollout] (expert) {专家标注\\$a^* = \pi^*(o)$};
\node[data, below=of policy] (dataset) {聚合数据集\\$\mathcal{D}_i$};
\node[block, fill=orange!10, left=1.5cm of dataset] (retrain) {重新训练\\$\pi_{i+1}$};

\draw[arrow] (policy) -- (rollout);
\draw[arrow] (rollout) -- (expert);
\draw[arrow] (expert) -- (dataset);
\draw[arrow] (dataset) -- (retrain);
\draw[arrow] (retrain) |- (policy);

\node[font=\scriptsize, color=gray] at (3.0, -2.5) {迭代 $i = 1, 2, \ldots, N$};
\end{tikzpicture}
\caption{DAgger数据聚合闭环流程}
\label{fig:dagger_loop}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.92\textwidth]{Image/图2-2_DAgger迭代式数据聚合全流程.png}
\caption{DAgger迭代式数据聚合全流程示意：上层为环境交互阶段，混合策略$\hat{\pi}_i = \beta_i \pi^* + (1-\beta_i)\pi_i$在环境中采集轨迹并由专家$\pi^*$修正标注；中层为数据聚合阶段，新采集数据$\mathcal{D}_{\text{new}}$与历史数据集$\mathcal{D}_i$合并；下层为训练更新阶段，以聚合数据集重训策略$\pi_{i+1}$。右侧对比图展示BC误差$O(T^2)$增长与DAgger误差$O(1)$收敛的理论差异}
\label{fig:dagger_detail}
\end{figure}

DAgger的理论分析表明，
经过$N$轮迭代后策略的期望损失上界降至$O(1)$：
\begin{equation}
  J(\hat{\pi}_N) \le J(\pi^*) + O\left(\frac{1}{N}\right)T \epsilon_N
\end{equation}
其中$\epsilon_N$为第$N$轮最优策略在聚合分布上的损失。
这意味着DAgger理论上能够消除$O(T^2)$的累积效应。

后续变体包括SafeDAgger\cite{Zhang2016QueryDAgger}（基于安全代理判断是否查询专家）、HG-DAgger\cite{Kelly2019HG_DAgger}（人机交互模式）等。
本文采用标准DAgger框架以保持方法简洁性，
具体工程实现细节见第3章。

\subsection{DAgger的工程化实现口径}

DAgger的理论优美，
但工程实现中有多个容易出错的细节需要明确：

\begin{itemize}
  \item $\beta$混合的实现方式：本文采用"状态级混合"，
    即在每个控制步以概率$\beta$执行专家动作、以概率$1-\beta$执行学生动作。
    另一种实现方式是"轨迹级混合"（前$\beta$比例的轨迹用专家采集），
    但状态级混合能更好地覆盖学生策略的错误状态；
     \item 专家标注的时机：无论实际执行的是专家还是学生动作，
    所有状态都由专家标注。
    这保证了每个状态都有正确的监督信号；
     \item 数据不平衡处理：随着DAgger轮次增加，
    新增数据量远小于初始BC数据。
    本文的处理方式是全量重训而非增量微调，
    以避免遗忘效应；
     \item 采集策略的选择：每轮新增数据偏重高速段（$\SI{9}{m/s}$、$\SI{12}{m/s}$各6条轨迹），
    因为这是BC基线最脆弱的区域。
     \end{itemize}


\section{视觉表征：CNN与ViT}

\subsection{卷积神经网络}

卷积神经网络（CNN）\cite{Lecun1998CNN} 凭借局部感受野、权重共享以及层级化特征提取，确立了计算机视觉表征的基础范式。
其中，VGG \cite{Simonyan2015VGG} 通过堆叠小型卷积核验证了网络深度的关键作用，
而 ResNet \cite{He2016ResNet} 引入的残差连接则有效解决了深层网络训练中的退化问题。
在早期的端到端无人机避障研究中，
CNN 是主流的视觉编码器方案 \cite{Loquercio2018DroNet,Sadeghi2017CAD2RL}。
然而，
CNN 在建模全局结构关系方面受限于其固有的局部运算机制：尽管通过多层堆叠可扩大理论感受野，
但研究表明其实际有效感受野（Effective Receptive Field）往往远小于输入图像尺寸 \cite{Lecun1998CNN}。
在复杂避障任务中，
这种局部性限制了模型捕捉跨区域长程依赖及远距离障碍物间空间逻辑关系的能力。

\subsection{视觉Transformer（ViT）}

Dosovitskiy等提出的Vision Transformer（ViT）\cite{Dosovitskiy2020ViT}将Transformer\cite{Vaswani2017Transformer}范式引入图像识别：将图像划分为固定大小的patch token，
经线性映射后输入标准Transformer编码器。
如图~\ref{fig:vit_patch}所示，
ViT通过自注意力机制建模任意patch对之间的全局依赖，
突破了CNN的感受野限制。

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
  >=Stealth,
  node distance=0.4cm,
]
% 输入图像
\node[draw, fill=blue!5, minimum width=2.4cm, minimum height=1.6cm] (img) at (0, 0) {};
% 网格线
\draw[gray, thin] (-0.8, -0.8) grid[step=0.4] (1.2, 0.8);
\node[font=\scriptsize] at (0, -1.2) {输入图像 ($H{\times}W$)};

% 箭头
\draw[->, thick] (1.6, 0) -- (2.4, 0);

% Patch tokens
\foreach \i in {0,...,5} {
  \node[draw, fill=orange!20, minimum width=0.35cm, minimum height=0.35cm] at (2.8+\i*0.45, 0.4) {};
}
\node[font=\scriptsize] at (4.0, -0.1) {Patch Tokens};
\node[font=\scriptsize, color=gray] at (4.0, -0.5) {$N = HW/P^2$};

% 箭头
\draw[->, thick] (5.6, 0.2) -- (6.4, 0.2);
\node[font=\scriptsize] at (6.0, -0.2) {线性嵌入};

% Transformer编码器
\node[draw, fill=orange!10, rounded corners=3pt, minimum width=2.2cm, minimum height=1.6cm, align=center, font=\small] at (8.0, 0.2) {Transformer\\编码器\\(自注意力)};

% 箭头
\draw[->, thick] (9.3, 0.2) -- (10.0, 0.2);

% 输出
\node[draw, fill=green!10, rounded corners=3pt, minimum width=1.2cm, minimum height=0.8cm, align=center, font=\small] at (10.8, 0.2) {特征\\向量};
\end{tikzpicture}
\caption{ViT的patch token化与Transformer编码流程示意}
\label{fig:vit_patch}
\end{figure}

在四旋翼避障方向，
Xing等\cite{Xing2024VisionBackbone}系统比较了多种视觉backbone，
指出ViT在高速与泛化条件下具备明显优势。
后续DeiT\cite{Touvron2021DeiT}通过知识蒸馏在无需大规模预训练数据的条件下提升ViT的训练效率；
Swin Transformer\cite{Liu2021SwinTransformer}通过分层窗口注意力降低计算复杂度并引入多尺度特征；
MAE\cite{He2022MAE}与BEiT\cite{Bao2022BEiT}进一步探索了大规模自监督预训练方法。

\subsection{轻量化ViT的设计维度}

在端到端控制场景中，
视觉编码器的设计需要在表征能力与推理效率之间取得平衡。
影响ViT效率的核心参数是patch数量$N$：自注意力的计算复杂度为$O(N^2 \cdot d)$，
其中$d$为嵌入维度。
表~\ref{tab:vit_complexity}展示了不同分辨率与patch size组合下的token数量及其对推理效率的影响。

\begin{table}[htbp]
\centering
\caption{不同输入分辨率与Patch Size下的Token数量与注意力复杂度}
\label{tab:vit_complexity}
\zihao{5}
\begin{tabular}{ccccc}
\toprule
\textbf{输入分辨率} & \textbf{Patch Size} & \textbf{Token数} $N$ & \textbf{注意力复杂度} $O(N^2)$ & \textbf{相对复杂度} \\
\midrule
$60 \times 90$ & $16 \times 16$ & 21 & $441$ & $1.0\times$ \\
$60 \times 90$ & $8 \times 8$ & 84 & $7{,}056$ & $16\times$ \\
$120 \times 180$ & $16 \times 16$ & 84 & $7{,}056$ & $16\times$ \\
$120 \times 180$ & $8 \times 8$ & 337 & $113{,}569$ & $257\times$ \\
$224 \times 224$ & $16 \times 16$ & 196 & $38{,}416$ & $87\times$ \\
\bottomrule
\end{tabular}
\end{table}

本文选择$60 \times 90$输入分辨率配合两阶段卷积嵌入（而非标准patch嵌入），
使第一阶段token数为$16 \times 24 = 384$，
第二阶段下采样至$8 \times 12 = 96$，
在保留空间细节的同时控制计算量。
这一设计使得ViT编码器在NVIDIA RTX 4060 GPU上的推理延迟可控制在$\SI{5}{ms}$以内。
第3章将给出各模块的详细耗时分析。


\section{时序建模：RNN/LSTM与SSM}

\subsection{LSTM的流式优势与局限}

循环神经网络（RNN）\cite{Elman1990RNN}及其变体LSTM\cite{Hochreiter1997LSTM}通过门控机制选择性地保留与更新记忆状态，
是端到端控制中最早用于时序聚合的模型。
LSTM的单步递推形式为：
\begin{align}
  \mathbf{f}_t &= \sigma(\mathbf{W}_f [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_f) &\text{（遗忘门）} \\
  \mathbf{i}_t &= \sigma(\mathbf{W}_i [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_i) &\text{（输入门）} \\
  \mathbf{c}_t &= \mathbf{f}_t \odot \mathbf{c}_{t-1} + \mathbf{i}_t \odot \tanh(\mathbf{W}_c [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_c) &\text{（记忆更新）} \\
  \mathbf{o}_t &= \sigma(\mathbf{W}_o [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_o) &\text{（输出门）} \\
  \mathbf{h}_t &= \mathbf{o}_t \odot \tanh(\mathbf{c}_t) &\text{（隐状态）}
\end{align}

LSTM的优势在于天然支持流式递推推理：每步仅需输入当前观测并更新固定大小的隐状态$(\mathbf{h}_t, \mathbf{c}_t)$。
然而，
LSTM面临明确的局限：（1）长期依赖建模受限——虽然门控缓解了梯度消失，
但实际中有效记忆范围通常在50--200步\cite{Hochreiter1997LSTM}；
（2）训练效率低——序列依赖性阻碍并行化，
训练速度远慢于Transformer；
（3）部署状态管理敏感——隐状态$(\mathbf{h}_t, \mathbf{c}_t)$的管理同样面临第4章所讨论的一致性问题。

\subsection{结构化状态空间模型（S4）}

结构化状态空间模型（Structured State Space Models, SSMs）建立在经典控制理论的基础之上，通过连续时间线性常微分方程对序列数据进行建模 \cite{Gu2022S4}：
\begin{equation}
  \mathbf{h}'(t) = \mathbf{A}\mathbf{h}(t) + \mathbf{B}\mathbf{x}(t), \quad \mathbf{y}(t) = \mathbf{C}\mathbf{h}(t) + \mathbf{D}\mathbf{x}(t)
  \label{eq:ssm}
\end{equation}
式中，$\mathbf{h}(t) \in \mathbb{R}^{d_{\text{state}}}$ 表示随时间演化的隐状态向量，
$\mathbf{A} \in \mathbb{R}^{d_{\text{state}} \times d_{\text{state}}}$ 为状态转移矩阵，决定了系统的演化动力学；
$\mathbf{B} \in \mathbb{R}^{d_{\text{state}} \times 1}$ 为输入投影矩阵，控制输入信号对状态的影响；
$\mathbf{C} \in \mathbb{R}^{1 \times d_{\text{state}}}$ 为输出投影矩阵，负责从隐状态中重构输出特征。

为了解决长序列训练中的梯度问题，S4 \cite{Gu2022S4} 引入了 HiPPO \cite{Gu2020HiPPO} 矩阵对 $\mathbf{A}$ 进行特定的结构化初始化。
此后的 S5 \cite{Smith2023S5} 通过简化实现降低了计算复杂度，
而 DSS \cite{Gu2022DSS} 则进一步探索了对角化参数方案的有效性。

\subsection{从连续到离散的零阶保持（ZOH）推导}

鉴于现代计算硬件处理的是离散数据，
必须将连续时间的 SSM 方程离散化。
本研究采用零阶保持（Zero-Order Hold, ZOH）作为离散化策略，
该方法假设输入信号在采样时间间隔 $\Delta$ 内保持恒定。

考虑连续时间方程 $\mathbf{h}'(t) = \mathbf{A}\mathbf{h}(t) + \mathbf{B}\mathbf{x}(t)$，
在时间区间 $[t_k, t_{k+1})$ 内（其中 $t_{k+1} = t_k + \Delta$），
设输入 $\mathbf{x}(t) = \mathbf{x}_k$ 为常数。
该常微分方程在 $t_{k+1}$ 时刻的解析解可推导为：
\begin{equation}
  \mathbf{h}(t_{k+1}) = e^{\mathbf{A}\Delta} \mathbf{h}(t_k) + \left(\int_0^{\Delta} e^{\mathbf{A}\tau} d\tau \right) \mathbf{B} \mathbf{x}_k
\end{equation}

定义离散化后的状态转移矩阵 $\bar{\mathbf{A}} = e^{\mathbf{A}\Delta}$，
以及输入控制矩阵 $\bar{\mathbf{B}} = \left(\int_0^{\Delta} e^{\mathbf{A}\tau} d\tau \right) \mathbf{B} = \mathbf{A}^{-1}(e^{\mathbf{A}\Delta} - \mathbf{I})\mathbf{B}$，
则离散时间下的递推方程可写作：
\begin{equation}
  \mathbf{h}_k = \bar{\mathbf{A}} \mathbf{h}_{k-1} + \bar{\mathbf{B}} \mathbf{x}_k, \quad \mathbf{y}_k = \mathbf{C} \mathbf{h}_k
  \label{eq:ssm_discrete}
\end{equation}

式 (\ref{eq:ssm_discrete}) 揭示了 SSM 与循环神经网络（如 RNN、LSTM）在形式上的同构性：两者均遵循“当前状态 = 转移矩阵 $\times$ 上一状态 + 输入投影”的线性递推逻辑。
然而，SSM 具备显著的计算优势：
（1）矩阵 $\bar{\mathbf{A}}$ 可被设计为对角结构，从而支持通过并行扫描算法（Parallel Scan）实现高效训练 \cite{Gu2022S4}；
（2）作为连续时间模型的离散化近似，步长参数 $\Delta$ 赋予了模型适应不同采样频率的灵活性。

\subsection{Mamba的选择性机制}

在 S4 的基础上，Gu 与 Dao 提出的 Mamba 架构 \cite{Gu2023Mamba} 引入了核心的“选择性状态空间”（Selective State Space）机制。
该机制打破了传统 SSM 参数时不变（Time-Invariant）的限制，
使离散化参数 $\mathbf{B}_t, \mathbf{C}_t$ 及步长 $\Delta_t$ 能够根据当前输入 $\mathbf{x}_t$ 动态生成：
\begin{equation}
  \Delta_t = \text{softplus}(\mathbf{W}_\Delta \mathbf{x}_t + \mathbf{b}_\Delta), \quad
  \mathbf{B}_t = \mathbf{W}_B \mathbf{x}_t, \quad
  \mathbf{C}_t = \mathbf{W}_C \mathbf{x}_t
  \label{eq:mamba_selective}
\end{equation}

这一“输入依赖性”（Input-Dependent）赋予了模型细粒度的内容感知与控制能力，其物理直觉可解释为：

\begin{itemize}
  \item $\Delta_t$ 调节“记忆的时间跨度”：
    当 $\Delta_t$ 较大时，状态转移 $\bar{\mathbf{A}}_t = e^{\mathbf{A}\Delta_t}$ 的衰减加剧，
    意味着模型倾向于忽略历史信息，聚焦于当前输入；
    反之，较小的 $\Delta_t$ 则有助于长时记忆的保持。
  \item $\mathbf{B}_t$ 控制“信息的写入强度”：
    通过输入相关的 $\mathbf{B}_t$，模型能够有选择地过滤噪声，仅将当前输入中关键的特征维度写入隐状态。
  \item $\mathbf{C}_t$ 决定“状态的读取焦点”：
    动态的 $\mathbf{C}_t$ 允许模型根据当前上下文需求，从复杂的隐状态中精准提取最相关的信息分量。
\end{itemize}

在无人机避障控制场景中，这种选择性机制展现出天然的适配性：
当遭遇突发障碍物时，模型可自适应地增大 $\Delta_t$ 以提升对最新观测的敏感度，实现快速响应；
而在平稳飞行阶段，减小 $\Delta_t$ 则有助于利用长时历史信息平滑轨迹预测，抑制噪声干扰。

图~\ref{fig:mamba_overview} 展示了 SSM/Mamba 的三层架构总览及选择性机制的直觉解释。

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{Image/图2-3_SSM与Mamba三层架构总览.png}
\caption{SSM/Mamba 的三层架构总览。上层：连续时间状态空间方程 $\mathbf{h}'(t) = \mathbf{A}\mathbf{h}(t) + \mathbf{B}\mathbf{x}(t)$，其中 $\mathbf{A}$ 驱动状态演化，$\mathbf{B}$ 控制输入注入，$\mathbf{C}$ 负责状态读出；中层：基于零阶保持（ZOH）的离散化过程，将连续参数转化为离散递推形式 $\bar{\mathbf{A}} = e^{\mathbf{A}\Delta}$；下层：Mamba 的选择性机制，展示了参数 $\Delta_t$、$\mathbf{B}_t$、$\mathbf{C}_t$ 如何依赖输入 $\mathbf{x}_t$ 进行动态调制。右侧示意图类比了其自适应控制逻辑与 LSTM 门控机制的异同。}
\label{fig:mamba_overview}
\end{figure}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
  >=Stealth,
  block/.style={draw, rounded corners=3pt, minimum width=1.6cm, minimum height=0.8cm, align=center, font=\small},
  arrow/.style={->, thick, color=black!70},
  state/.style={draw, circle, minimum size=0.8cm, font=\small},
]
% 时间步 t-1
\node[block, fill=blue!10] (x0) at (0, 0) {输入 $t{-}1$};
\node[state, fill=orange!15] (h0) at (0, 1.5) {$\mathbf{h}_{t-1}$};
\node[block, fill=green!10] (y0) at (0, 3.0) {输出 $t{-}1$};
\draw[arrow] (x0) -- node[right, font=\scriptsize] {$\bar{\mathbf{B}}_{t-1}$} (h0);
\draw[arrow] (h0) -- node[right, font=\scriptsize] {$\mathbf{C}_{t-1}$} (y0);
% 时间步 t
\node[block, fill=blue!10] (x1) at (3.5, 0) {输入 $t$};
\node[state, fill=orange!15] (h1) at (3.5, 1.5) {$\mathbf{h}_{t}$};
\node[block, fill=green!10] (y1) at (3.5, 3.0) {输出 $t$};
\draw[arrow] (x1) -- node[right, font=\scriptsize] {$\bar{\mathbf{B}}_{t}$} (h1);
\draw[arrow] (h1) -- node[right, font=\scriptsize] {$\mathbf{C}_{t}$} (y1);
% 时间步 t+1
\node[block, fill=blue!10] (x2) at (7.0, 0) {输入 $t{+}1$};
\node[state, fill=orange!15] (h2) at (7.0, 1.5) {$\mathbf{h}_{t+1}$};
\node[block, fill=green!10] (y2) at (7.0, 3.0) {输出 $t{+}1$};
\draw[arrow] (x2) -- node[right, font=\scriptsize] {$\bar{\mathbf{B}}_{t+1}$} (h2);
\draw[arrow] (h2) -- node[right, font=\scriptsize] {$\mathbf{C}_{t+1}$} (y2);
% 状态传播
\draw[arrow, red!60, very thick] (h0) -- node[above, font=\scriptsize, color=red!60] {$\bar{\mathbf{A}}$} (h1);
\draw[arrow, red!60, very thick] (h1) -- node[above, font=\scriptsize, color=red!60] {$\bar{\mathbf{A}}$} (h2);

\node[font=\scriptsize, color=red!60] at (3.5, -0.8) {$\mathbf{h}_t = \bar{\mathbf{A}}\mathbf{h}_{t-1} + \bar{\mathbf{B}}_t\mathbf{x}_t$, \quad $\mathbf{y}_t = \mathbf{C}_t\mathbf{h}_t$};
\end{tikzpicture}
\caption{SSM/Mamba 离散化后的状态更新机制。下标 $t$ 强调了参数 $\bar{\mathbf{B}}_t$ 与 $\mathbf{C}_t$ 随输入动态变化的选择性特性。}
\label{fig:ssm_block}
\end{figure}

如图~\ref{fig:ssm_block} 所示，离散化后的 SSM 在形式上表现为线性递推，这与 LSTM 等循环神经网络结构高度相似。
最新的研究工作 Mamba-2 \cite{Dao2024Mamba2} 进一步揭示了这种结构化状态空间模型与 Transformer 注意力机制之间的数学对偶性，
从而在理论层面统一了序列建模的两种主流范式。

\subsection{SSM对控制任务的意义}

表~\ref{tab:ssm_control_map}从四个维度分析了SSM特性与控制任务需求之间的映射关系。

\begin{table}[htbp]
\centering
\caption{SSM特性与高速避障控制需求的映射}
\label{tab:ssm_control_map}
\zihao{5}
\begin{tabular}{p{2.5cm}p{4.5cm}p{5.0cm}}
\toprule
\textbf{SSM特性} & \textbf{技术含义} & \textbf{对控制任务的价值} \\
\midrule
线性递推 & $O(n)$复杂度，流式推理友好 & 满足实时控制频率约束 \\
选择性机制 & $\Delta_t, \mathbf{B}_t, \mathbf{C}_t$依赖输入 & 自适应调节观测噪声抑制强度 \\
固定大小隐状态 & 状态维度不随序列长度增长 & 内存占用可预测，适合嵌入式部署 \\
连续时间参数化 & $\bar{\mathbf{A}} = e^{\mathbf{A}\Delta}$ & 对不等间距控制步自然适配 \\
\bottomrule
\end{tabular}
\end{table}

如图~\ref{fig:attn_vs_ssm}所示，
自注意力机制的$O(n^2)$复杂度与SSM的$O(n)$复杂度形成鲜明对比，
这一效率优势对实时控制至关重要。

\begin{figure}[htbp]
\centering
\begin{tikzpicture}
\begin{axis}[
  width=7.5cm, height=4.5cm,
  xlabel={序列长度 $n$},
  ylabel={相对计算量},
  xmin=0, xmax=100,
  ymin=0, ymax=10000,
  xtick={0,25,50,75,100},
  legend pos=north west,
  legend style={font=\small},
  grid=major,
  grid style={gray!20},
]
\addplot[domain=0:100, samples=50, thick, color=red!70, dashed] {x^2};
\addlegendentry{Attention $O(n^2)$}
\addplot[domain=0:100, samples=50, thick, color=blue!70] {x*30};
\addlegendentry{SSM $O(n)$}
\addplot[domain=0:100, samples=50, thick, color=green!60!black, dashdotted] {x*x*0.3 + x*10};
\addlegendentry{LSTM $O(n \cdot d^2)$}
\end{axis}
\end{tikzpicture}
\caption{Attention、SSM与LSTM的序列长度--计算量关系对比（示意）}
\label{fig:attn_vs_ssm}
\end{figure}


\section{MambaVision：混合Mamba-Transformer视觉骨干}

MambaVision \cite{Hatamizadeh2025MambaVisionCVPR} 提出了一种专为视觉任务定制的混合架构，
旨在解决纯 SSM 模型在全局上下文建模上的先天不足 \cite{Zhu2024VisionMamba,Liu2024VMamba}。
该工作对 Mamba 的原生范式进行了针对性的重构与扩展：
首先，在微观设计上，
该模型移除了 SSM 中的因果卷积限制，代之以标准的二维卷积以适应图像的空间属性，
并引入了一个不含 SSM 的对称分支（Symmetric Branch），
通过拼接（Concatenation）而非门控机制来增强特征的表示能力 \cite{Hatamizadeh2025MambaVisionCVPR}；
其次，在宏观架构上，
MambaVision 采用了分层设计：
前两个阶段利用 CNN 残差块进行快速的高分辨率特征提取，
而在深层阶段（Stage 3 \& 4）则采用了“Mamba 前置、Attention 后置”的混合策略 \cite{Hatamizadeh2025MambaVisionCVPR}。
消融实验表明，
在深层网络的末端引入自注意力（Self-Attention）块，
能够以极小的计算代价显著补偿 SSM 在长程空间依赖（Long-range Spatial Dependency）捕捉上的短板 \cite{Hatamizadeh2025MambaVisionCVPR}。
得益于此，MambaVision 在 ImageNet 分类及 COCO 检测任务上均取得了优于同量级纯 ViT 及纯 Mamba 模型的帕累托最优解（Pareto Front）\cite{Hatamizadeh2025MambaVisionCVPR}。

与之形成鲜明对比的是 Vision Mamba (Vim) \cite{Zhu2024VisionMamba}，
该工作代表了“纯 SSM”视觉骨干的设计路线。
Vim 摈弃了注意力机制，
转而利用双向状态空间模型（Bidirectional SSM）对图像序列进行正反向扫描，
试图在不引入 Transformer 的前提下实现全图上下文的覆盖 \cite{Zhu2024VisionMamba}。

\section{仿真平台与数据来源}

\subsection{Flightmare仿真平台}

本文所有实验在Flightmare高保真仿真平台\cite{Song2021Flightmare}中完成。
Flightmare的设计强调物理引擎与渲染引擎的解耦：物理仿真可以在不启动渲染的情况下以极高速率运行（用于大规模数据生成），
也可以启动渲染以支持视觉观测生成。
与AirSim\cite{Shah2018AirSim}和RotorS\cite{Furrer2016RotorS}等其他无人机仿真器相比，
Flightmare以"物理--渲染解耦"的设计在数据生成效率上具有显著优势。
Agilicious\cite{Foehn2022Agilicious}提供了开放软硬件一体化平台，
覆盖从MPC到神经网络控制的系统化验证。

\subsection{评测环境}

评测环境包含两类障碍分布，
如表~\ref{tab:env_config}所示：

\begin{table}[htbp]
\centering
\caption{评测环境配置}
\label{tab:env_config}
\zihao{5}
\begin{tabular}{p{2.5cm}p{2.5cm}p{6.0cm}}
\toprule
\textbf{环境名称} & \textbf{分布类型} & \textbf{障碍特征} \\
\midrule
Spheres & 同分布（ID） & 三维空间中随机分布的球体障碍，训练数据在该环境中生成。障碍半径与密度参数化控制。 \\
Trees & 分布外（OOD） & 树状结构障碍：细长圆柱模拟树干 + 半球冠层。策略从未在该环境中训练，测试零样本迁移能力。 \\
\bottomrule
\end{tabular}
\end{table}

设置两类环境的目的是分别评估策略的"训练分布内性能"和"分布外泛化能力"。
Trees环境的独特挑战在于：（1）树干在低分辨率深度图中仅占少数像素，
容易遗漏；
（2）冠层的形状与训练分布差异大，
可能导致距离估计偏差。

两类评测环境的实拍截图如图~\ref{fig:env_screenshots}所示。

\begin{figure}[htbp]
\centering
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Image/图2-4a_Spheres环境实拍同分布.png}
\centerline{(a) Spheres环境（同分布）}
\end{minipage}
\hfill
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=\textwidth]{Image/图2-4b_Trees环境实拍分布外.png}
\centerline{(b) Trees环境（分布外）}
\end{minipage}
\caption{Flightmare仿真平台中两类评测环境的实拍截图。(a) Spheres环境：三维空间中随机分布不同半径的球体障碍，训练数据在该环境中生成；(b) Trees环境：由树干与冠层构成的自然场景，策略从未在此环境中训练，用于测试零样本迁移泛化能力}
\label{fig:env_screenshots}
\end{figure}

\subsection{特权信息专家策略}

训练数据由特权信息专家策略在Spheres环境中生成。
与端到端策略不同，
专家策略在每个控制步可访问完整环境信息（无人机精确位置/速度、所有障碍物的位置/几何参数），
通过候选速度采样与碰撞检测生成高质量速度指令。
算法~\ref{alg:expert}给出专家策略的伪代码。

\begin{algorithm}[htbp]
\caption{特权信息专家策略}
\label{alg:expert}
\begin{algorithmic}[1]
\Require 无人机状态 $(\mathbf{p}_t, \mathbf{v}_t, q_t)$，障碍集合 $\mathcal{O}_{\text{env}}$，目标速度 $v^{\text{target}}$
\Ensure 专家速度指令 $\mathbf{v}_t^*$
\State \textbf{// 候选速度采样}
\State $\mathcal{V}_{\text{cand}} \leftarrow$ 在目标速度方向锥体内均匀采样 $K$ 个候选方向
\For{每个候选方向 $\hat{\mathbf{d}}_k \in \mathcal{V}_{\text{cand}}$}
  \State 构造候选速度 $\mathbf{v}_k = v^{\text{target}} \cdot \hat{\mathbf{d}}_k$
  \State \textbf{// 碰撞检测与安全裕度评估}
  \State $c_k \leftarrow \min_{\mathbf{o} \in \mathcal{O}_{\text{env}}} \text{clearance}(\mathbf{p}_t + \mathbf{v}_k \cdot \Delta t_{\text{lookahead}}, \mathbf{o})$
  \State \textbf{// 代价函数：安全性 + 目标方向对齐 + 平滑性}
  \State $\text{cost}_k \leftarrow -\alpha_1 c_k + \alpha_2 \|\hat{\mathbf{d}}_k - \hat{\mathbf{x}}\|_2 + \alpha_3 \|\mathbf{v}_k - \mathbf{v}_{t-1}^*\|_2$
\EndFor
\State $k^* \leftarrow \arg\min_k \text{cost}_k$
\State \Return $\mathbf{v}_t^* = \mathbf{v}_{k^*}$
\end{algorithmic}
\end{algorithm}

表~\ref{tab:expert_params}给出专家策略的超参数配置。

\begin{table}[htbp]
\centering
\caption{特权信息专家策略超参数}
\label{tab:expert_params}
\zihao{5}
\begin{tabular}{lcc}
\toprule
\textbf{参数} & \textbf{符号} & \textbf{数值} \\
\midrule
候选方向采样数 & $K$ & 128 \\
前视时间 & $\Delta t_{\text{lookahead}}$ & $\SI{0.5}{s}$ \\
安全裕度权重 & $\alpha_1$ & 1.0 \\
方向对齐权重 & $\alpha_2$ & 0.3 \\
平滑性权重 & $\alpha_3$ & 0.1 \\
采样锥体半角 & -- & $60^\circ$ \\
\bottomrule
\end{tabular}
\end{table}

\subsection{数据采集管线}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
  >=Stealth,
  node distance=0.6cm and 0.8cm,
  block/.style={draw, rounded corners=3pt, minimum width=2.4cm, minimum height=0.9cm, align=center, font=\small},
  arrow/.style={->, thick, color=black!70},
  data/.style={draw, rounded corners=3pt, fill=yellow!10, minimum width=2.4cm, minimum height=0.9cm, align=center, font=\small},
]
\node[block, fill=blue!10] (scene) {场景随机化\\(Spheres环境)};
\node[block, fill=green!10, right=of scene] (expert) {特权信息\\专家策略};
\node[block, fill=orange!10, right=of expert] (sim) {Flightmare\\闭环仿真};
\node[data, right=of sim] (traj) {轨迹数据\\$(D_t, s_t, a_t^*)$};
\node[data, below=0.8cm of traj] (dataset) {训练数据集\\(585条轨迹)};

\draw[arrow] (scene) -- (expert);
\draw[arrow] (expert) -- (sim);
\draw[arrow] (sim) -- (traj);
\draw[arrow] (traj) -- (dataset);

\node[font=\scriptsize, color=gray] at (5.5, -2.0) {专家可访问完整环境信息（位置、速度、障碍几何）};
\end{tikzpicture}
\caption{基于Flightmare与特权信息专家的数据采集管线}
\label{fig:data_pipeline}
\end{figure}

如图~\ref{fig:data_pipeline}所示，
训练数据由特权信息专家在Spheres环境中生成。
每条轨迹包含深度图像$D_t$、无人机状态$s_t$与专家速度指令$a_t^*$的时间序列。
训练数据集包含约585条专家轨迹，
覆盖5个速度档（$\SI{3}{m/s}$--$\SI{12}{m/s}$），
轨迹长度在200--800步之间。
注意，
Trees环境不参与任何训练数据的生成，
仅用于零样本OOD评测。


\section{评测协议与指标}

本节固定全篇统一的评测协议与指标定义。
后续各章实验直接引用本节表格与定义。

\subsection{统一评测协议}

统一评测协议如表~\ref{tab:eval_protocol_unified}所示。

\begin{table}[htbp]
\centering
\caption{统一评测协议}
\label{tab:eval_protocol_unified}
\zihao{5}
\begin{tabular}{lc}
\toprule
\textbf{参数} & \textbf{设置} \\
\midrule
目标速度档位 & 3, 5, 7, 9, 12 m/s \\
每档试验次数 & 10次 \\
回合终止距离 & 沿$X$轴 58--60 m \\
超时限制 & $\tau_{\max} = \SI{40}{s}$ \\
碰撞处理 & 不终止回合，持续记录 \\
状态管理 & KeepState（回合级重置） \\
测试环境 & Spheres（ID） + Trees（OOD） \\
随机种子 & 固定（PyTorch + NumPy + CUDA确定性） \\
硬件配置 & NVIDIA RTX 4060 GPU (8GB) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{指标定义}

表~\ref{tab:metric_def}给出了本文使用的所有评测指标的严格定义。

\begin{table}[htbp]
\centering
\caption{评测指标定义与计算口径}
\label{tab:metric_def}
\zihao{5}
\begin{tabular}{p{2.5cm}p{5.5cm}p{2.5cm}p{2.0cm}}
\toprule
\textbf{指标名称} & \textbf{定义} & \textbf{单位} & \textbf{统计方式} \\
\midrule
全程碰撞率 (Collision Rate) & $\sum_{t=1}^{T}\mathbb{1}[\text{collision}_t=1] / T$ & \% & 10次均值$\pm$std \\
碰撞事件次数 (Collision Count) & 碰撞标志上升沿计数 & 次/回合 & 10次均值$\pm$std \\
成功率 (Success Rate) & 超时限内到达终点的回合比例 & \% & 10次比例 \\
指令抖动 (Command Jerk) & $\|\mathbf{v}_t - \mathbf{v}_{t-1}\|_2$ 回合内均值 & m/s & 10次均值$\pm$std \\
推理时间 & 单步模型前向推理耗时 & ms & 中位数 \\
横向漂移 (Mean Y Drift) & $\frac{1}{T}\sum_{t=1}^{T}|y_t|$ & m & 10次均值 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{指标计算伪代码}

为确保评测指标的计算可复现，
本节给出关键指标的伪代码实现。

碰撞事件次数的计算采用上升沿检测：
\begin{equation}
  \text{Collision Count} = \sum_{t=2}^{T} \mathbb{1}[\text{collision}_t = 1 \wedge \text{collision}_{t-1} = 0]
  \label{eq:collision_count_ch2}
\end{equation}

\begin{algorithm}[htbp]
\caption{碰撞事件次数计算（上升沿检测）}
\label{alg:collision_count}
\begin{algorithmic}[1]
\Require 碰撞标志序列 $\texttt{collision}[1..T] \in \{0, 1\}^T$
\Ensure 碰撞事件次数 $\texttt{count}$
\State $\texttt{count} \leftarrow 0$
\For{$t = 2$ \textbf{to} $T$}
  \If{$\texttt{collision}[t] = 1$ \textbf{and} $\texttt{collision}[t-1] = 0$}
    \State $\texttt{count} \leftarrow \texttt{count} + 1$ \Comment{检测到上升沿}
  \EndIf
\EndFor
\State \Return $\texttt{count}$
\end{algorithmic}
\end{algorithm}

如图~\ref{fig:collision_edge}所示，
连续碰撞帧视为同一次碰撞事件，
仅统计上升沿以避免重复计数。

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
  >=Stealth,
]
% 时间轴
\draw[->, thick] (0, 0) -- (12, 0) node[right, font=\small] {时间 $t$};
\draw[->, thick] (0, 0) -- (0, 1.8) node[above, font=\small] {碰撞标志};

% 碰撞信号
\draw[very thick, blue!70] (0, 0) -- (2, 0) -- (2, 1.2) -- (4, 1.2) -- (4, 0) -- (7, 0) -- (7, 1.2) -- (8.5, 1.2) -- (8.5, 0) -- (11, 0);

% 上升沿标记
\draw[->, red!70, very thick] (2, -0.5) -- (2, 0);
\node[font=\scriptsize, color=red!70] at (2, -0.8) {上升沿1};
\draw[->, red!70, very thick] (7, -0.5) -- (7, 0);
\node[font=\scriptsize, color=red!70] at (7, -0.8) {上升沿2};

% 标注
\node[font=\scriptsize, color=blue!70] at (3, 1.6) {碰撞事件1};
\node[font=\scriptsize, color=blue!70] at (7.75, 1.6) {碰撞事件2};

% Collision Count
\node[draw, rounded corners=2pt, fill=yellow!10, font=\small] at (6, -1.6) {Collision Count = 2（仅统计上升沿）};
\end{tikzpicture}
\caption{碰撞事件次数的上升沿检测计算示意}
\label{fig:collision_edge}
\end{figure}

\begin{algorithm}[htbp]
\caption{Command Jerk计算}
\label{alg:jerk_calc}
\begin{algorithmic}[1]
\Require 速度指令序列 $\mathbf{v}[1..T] \in \mathbb{R}^{T \times 3}$
\Ensure 平均Jerk $\bar{J}$
\State $\texttt{jerk\_sum} \leftarrow 0$
\For{$t = 2$ \textbf{to} $T$}
  \State $\texttt{jerk\_sum} \leftarrow \texttt{jerk\_sum} + \|\mathbf{v}[t] - \mathbf{v}[t-1]\|_2$
\EndFor
\State $\bar{J} \leftarrow \texttt{jerk\_sum} / (T - 1)$
\State \Return $\bar{J}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[htbp]
\caption{横向漂移（Mean Y Drift）计算}
\label{alg:drift_calc}
\begin{algorithmic}[1]
\Require 位置序列 $\mathbf{p}[1..T] \in \mathbb{R}^{T \times 3}$
\Ensure 平均横向漂移 $\bar{D}_y$
\State $\bar{D}_y \leftarrow \frac{1}{T} \sum_{t=1}^{T} |p_y[t]|$ \Comment{$p_y$为$Y$轴分量}
\State \Return $\bar{D}_y$
\end{algorithmic}
\end{algorithm}

\subsection{统计显著性与不确定性报告}

本文评测中每个配置进行10次独立试验（固定种子但不同初始位置），
报告均值$\pm$标准差。
采用这一方案而非更复杂的统计检验（如$t$-test或bootstrap置信区间）的原因在于：

\begin{enumerate}
  \item 样本量限制：每档仅10次试验，
    样本量不满足正态性假设的可靠性要求；
     \item 效应量显著：本文的主要对比（如KeepState vs ResetState的碰撞率差异为$0\%$对$90\%$）效应量远超统计噪声；
     \item 标准差的信息量：标准差直接反映策略行为的稳定性，
    是衡量工程部署可靠性的关键指标——高标准差意味着策略行为不可预测，
    即使均值尚可，
    工程上也不可接受。
     \end{enumerate}

\subsection{评测可审计规范}

\begin{enumerate}
\item 为确保实验结论的可复现性与可追溯性，本文建立以下评测可审计规范：
\item 随机种子固定：所有实验固定随机种子（包括PyTorch、NumPy、CUDA确定性模式与环境初始化种子）；
 \item 环境参数记录：每次评测自动记录环境类型、障碍密度参数、目标速度档位与回合终止条件等关键配置；
 \item 状态重置时机：明确记录序列模型内部状态的重置时机（仅在回合边界），
并通过运行时断言确保回合内状态的连续传播（详见第4章）；
 \item 版本号固化：记录策略网络权重文件的哈希值、代码版本号与依赖库版本；
 \item 控制周期分布：记录每次试验中所有控制步的$\Delta t$时间间隔分布，
用于排除系统负载差异造成的混淆因素。
 \end{enumerate}

上述规范贯穿本文所有实验，
确保评测结论不受实现细节污染。


\section{相关工作综述}

\subsection{端到端视觉飞行控制}

端到端控制范式致力于构建从原始感知数据到控制指令的直接映射，其发展呈现出从简单场景导航向极限敏捷机动演进的趋势。
早期的探索性工作如 DroNet\cite{Loquercio2018DroNet}，成功将卷积神经网络（CNN）应用于城市环境的自主导航，初步验证了视觉模仿学习的可行性。
随后，为了突破现实训练数据的获取瓶颈，
CAD2RL\cite{Sadeghi2017CAD2RL} 与 Deep Drone Racing\cite{Kaufmann2018DeepDroneRacing} 率先证实了在仿真环境中训练并迁移至现实世界（Sim-to-Real）的有效性。
在避障策略方面，Gandhi 等\cite{Gandhi2017CollisionDrone} 提出了一种基于碰撞数据的自监督学习机制，利用无人机的“试错”经历来提升安全性。

随着对飞行性能要求的提升，研究重心逐渐转向高动态机动。
Kaufmann 等的 Deep Drone Acrobatics\cite{Kaufmann2020DeepDroneAcrobatics} 将端到端方法扩展至翻滚等极限动作；
Loquercio 等\cite{Loquercio2021HighSpeedWild} 确立了“特权专家蒸馏 + 域随机化”的标准范式，实现了野外环境下的高速穿越；
Swift 系统\cite{Kaufmann2023SwiftNature} 更是结合深度强化学习，在竞速对抗任务中达到了超越人类冠军的水平。
此外，Pan 等\cite{Pan2018AgileAutonomous} 验证了深度模仿学习在自动驾驶场景下的敏捷性，
而 Shah 等\cite{Shah2023GNM} 提出的通用导航模型（GNM）则进一步探索了跨机器人平台的通用端到端策略。
上述工作共同奠定了当前主流的“仿真学习--专家指导--域迁移”的技术基石。

\subsection{模块化自主飞行}

传统的模块化自主飞行系统通常遵循“感知--规划--控制”的分层架构。
在感知与状态估计层面，
ORB-SLAM 系列\cite{MurArtal2017ORBSLAM2,Campos2021ORBSLAM3} 确立了稀疏特征法的标杆，
LSD-SLAM\cite{Engel2014LSDSLAM} 探索了直接法在大尺度环境下的应用，
而 VINS-Mono\cite{Qin2018VINSMONO} 则通过视觉惯性紧耦合显著提升了鲁棒性。
Cadena 等\cite{Cadena2016SLAMSurvey} 的综述文章系统总结了 SLAM 技术从滤波器时代迈向鲁棒感知时代的演进历程。
在规划与控制层面，
基于梯度的轨迹优化（如 Minimum Snap\cite{Mellinger2011MinSnapTrajectory} 及其多项式扩展\cite{Richter2016MinSnapPoly}）与基于采样的 RRT*\cite{Karaman2011SamplingOptimal} 构成了经典理论基础；
非线性模型预测控制（NMPC）\cite{Kamel2017NMPC,Neunert2016MPC_Quadrotor} 则进一步提升了四旋翼在动态约束下的轨迹跟踪性能。

国内学者在该领域亦做出了系统性贡献。
高翔等\cite{Gao2019SLAMSurvey} 深入分析了特征法与直接法在精度与效率上的权衡，并前瞻性地指出语义融合是下一阶段的关键突破口；
张弓等\cite{Zhang2018VIOSLAM} 与吴潇等\cite{Wu2022QuadSLAM} 则分别针对高动态鲁棒性与机载计算受限场景，详细论证了紧耦合 VIO 与轻量化 SLAM 的部署优势。
在轨迹规划领域，
Zhou 等提出的 Fast-Planner\cite{Zhou2019FastPlanner} 及其后续 EGO-Planner\cite{Zhou2021EGOPlanner} 代表了显著的技术跨越：
后者成功移除了对欧几里得符号距离场（ESDF）的依赖，通过直接计算障碍点云的碰撞梯度，将规划效率提升了一个数量级。
此外，何承坤等\cite{He2021QuadTrajectory} 对比了多项式优化与 B 样条技术在实时性上的折中，
张涛等\cite{Zhang2020AutoPilotSurvey} 与刘小雄等\cite{Liu2020QuadControl} 的综述文章则从系统架构层面指出，
尽管模块化方法在结构化场景中表现成熟，
但在高速密集障碍环境中，其固有的感知延迟与模块间误差累积问题仍是制约性能的瓶颈。

\subsection{安全性与部署侧约束}

随着学习型控制方法的兴起，如何通过形式化手段保障系统的安全性成为研究热点。
Brunke 等\cite{Brunke2022SafeLearningReview} 对安全学习控制路线进行了系统梳理。
目前的主流方案包括：利用控制障碍函数（CBF）\cite{Ames2019CBFSurvey} 构建安全边界，
并将其嵌入强化学习框架以约束探索行为\cite{Cheng2019RLwithCBF}；
以及基于模型预测安全控制（MPSC）\cite{Wabersich2018MPSC} 的预测滤波机制。
Fisac 等\cite{Fisac2019SafeRL} 与 Garc\'{i}a 等\cite{GarciaPineda2015SafeRLSurvey} 则分别建立了通用的安全学习框架与理论综述。

针对无人机平台的特殊部署约束，国内研究重点关注算法的实时性与迁移鲁棒性。
雷志勇等\cite{Lei2020DRLAvoidance} 验证了深度 Q 网络（DQN）在稀疏激光雷达输入下的实时决策能力；
严旭等\cite{Yan2021DRLObstacle} 提出深度图与惯性数据融合方案，有效提升了三维动态场景下的避障成功率。
在训练算法选择上，李超等\cite{Li2022RLUAV} 的对比研究表明，近端策略优化（PPO）在连续动作空间任务中具有最优的稳定性与收敛速度。
然而，正如陈杰等\cite{Chen2023DRLDroneReview} 所指出的，仿真到实体（Sim-to-Real）的鸿沟仍是限制 DRL 广泛落地的核心难题。
朱福利等\cite{Zhu2021DeepLearningUAV} 则从边缘计算视角强调，模型压缩与轻量化推理是实现机载实时感知不可或缺的关键技术。

\subsection{Sim-to-Real迁移}

Sim-to-Real 迁移是弥合仿真训练与物理部署差距的关键桥梁。
其核心挑战在于缩小感知与动力学的分布偏移。
Tobin 等\cite{Tobin2017DomainRandomization} 首创了域随机化（Domain Randomization）方法，通过在仿真中大幅扰动纹理与光照等视觉属性，使模型习得对视觉噪声的“不变性”；
Peng 等\cite{Peng2018SimtoRealRL} 与 Molchanov 等\cite{Molchanov2019SimRL} 随后将这一思想扩展至动力学参数，实现了策略向不同物理平台的鲁棒迁移。
Zhao 等\cite{Zhao2020SimtoReal} 对此进行了全面综述。
本文主要在 Flightmare 高保真仿真环境中进行算法验证，
关于物理实机部署中的 Sim-to-Real 迁移策略，将在第 5 章作为未来工作方向进行讨论。

\subsection{方法谱系总结}

表~\ref{tab:route_compare}从四个维度对主要技术路线进行横向对比。

\begin{table}[htbp]
\centering
\caption{高速端到端视觉避障相关技术路线对比}
\label{tab:route_compare}
\zihao{5}
\begin{tabular}{p{1.5cm}p{2.8cm}p{2.8cm}p{2.5cm}p{2.5cm}}
\toprule
\textbf{对比维度} & \textbf{路线A} & \textbf{路线B} & \textbf{A的优势} & \textbf{B的优势} \\
\midrule
系统范式 &
模块化（感知--规划--控制） &
端到端（视觉$\to$控制） &
可解释、可验证 &
低延迟、架构简洁 \\
\midrule
训练方法 &
行为克隆（BC） &
DAgger/强化学习 &
训练稳定、样本高效 &
闭环分布覆盖更好 \\
\midrule
时序建模 &
LSTM/RNN &
SSM（Mamba） &
工程成熟、流式支持 &
线性复杂度、选择性机制 \\
\midrule
视觉编码 &
ViT &
MambaVision &
全局注意力、强表征 &
效率更优、架构统一 \\
\bottomrule
\end{tabular}
\end{table}


\section{小结：设计需求}

综合本章的预备知识与相关工作分析，
对后续创新章节提出以下设计需求：

\begin{itemize}
  \item 需要低延迟的时序建模能力，
    以支撑高速闭环控制（$\rightarrow$ 第3章：ViT+Mamba）；
     \item 需要闭环数据增强机制以缓解BC的分布偏移（$\rightarrow$ 第3章：DAgger）；
     \item 需要部署侧平滑约束以控制敏捷性带来的指令抖动（$\rightarrow$ 第3章：RACS）；
     \item 需要严格的流式部署一致性验证机制（$\rightarrow$ 第4章：状态生命周期管理）；
     \item 需要在安全/平滑/延迟/显存四维做统一对比，
    评估SSM视觉骨干的可行性（$\rightarrow$ 第5章：MambaVision）；
     \item 需要可复现的指标口径与评测可审计规范（$\rightarrow$ 本章表~\ref{tab:eval_protocol_unified}与表~\ref{tab:metric_def}）。
     \end{itemize}
